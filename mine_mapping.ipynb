{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd5743b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove this\n",
    "#! pip install torch==2.3 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "! pip install --upgrade torch torchvision==0.15.0+cu118 --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "#! pip install stacchip\n",
    "#! pip install geopandas\n",
    "\n",
    "! pip install contextily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9960547-640d-425c-8180-fc5523a80e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "\n",
    "import geoarrow.pyarrow as ga\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pystac_client\n",
    "import requests\n",
    "import torch\n",
    "import yaml\n",
    "from box import Box\n",
    "\n",
    "import stacchip.indexer\n",
    "from stacchip.chipper import Chipper\n",
    "from stacchip.indexer import Sentinel2Indexer\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "from shapely.geometry import Polygon\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, reduce, repeat\n",
    "from torch import nn\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "from src.backbone import Transformer\n",
    "from src.factory import DynamicEmbedding\n",
    "from src.utils import posemb_sincos_2d_with_gsd\n",
    "\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "os.environ[\"TORCH_CUDNN_V8_API_DISABLED\"] = \"1\"\n",
    "\n",
    "#from src.model import ClayMAEModule\n",
    "\n",
    "import contextily as cx\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "\n",
    "from shapely.geometry import box\n",
    "\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from shapely import wkb\n",
    "\n",
    "import stackstac\n",
    "import pystac_client\n",
    "import numpy as np\n",
    "import dask\n",
    "from rasterio.enums import Resampling\n",
    "from rasterstats import zonal_stats\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import shapely\n",
    "from shapely.geometry import Polygon\n",
    "from rasterio.features import rasterize\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1080a3",
   "metadata": {},
   "source": [
    "### Load mining data and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501b0e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mine polygons from Tang dataset and remove Z coordinate\n",
    "mine_poly = gpd.read_file('data/mine_poly_shp').to_crs(epsg=4326)\n",
    "\n",
    "def convert_3D_2D(geometry):\n",
    "    '''\n",
    "    Takes a GeoSeries of 3D Multi/Polygons (has_z) and returns a list of 2D Multi/Polygons\n",
    "    From : https://gist.github.com/rmania/8c88377a5c902dfbc134795a7af538d8\n",
    "    '''\n",
    "    new_geo = []\n",
    "    for p in geometry:\n",
    "        if p.has_z:\n",
    "            if p.geom_type == 'Polygon':\n",
    "                lines = [xy[:2] for xy in list(p.exterior.coords)]\n",
    "                new_p = Polygon(lines)\n",
    "                new_geo.append(new_p)\n",
    "        else:\n",
    "            print('no z')\n",
    "    return new_geo\n",
    "\n",
    "mine_poly.geometry = convert_3D_2D(mine_poly.geometry) \n",
    "\n",
    "print('Number of mine features:')\n",
    "print(len(mine_poly))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cf94af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mine point data\n",
    "mine_pts = gpd.read_file('data/pitlakes').to_crs(epsg=4326)\n",
    "\n",
    "# Print info about dataset\n",
    "print(mine_pts['Material'].value_counts())\n",
    "print(len(mine_pts))\n",
    "print(mine_pts['ID'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bf81598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Aquarry QA'ed data\n",
    "psql_bboxes = gpd.read_file('data/aquarry_psql_layer.csv')\n",
    "psql_bboxes = gpd.GeoDataFrame(psql_bboxes,geometry = psql_bboxes['field_8'].apply(wkb.loads),crs='epsg:4326') # Convert WKB geometry\n",
    "\n",
    "psql_bboxes.columns = ['cc', 'objectid', 'score', 'label', 'dataset', 'area', 'volume', 'wkb_geometry','field_9', 'category', 'field_11', 'geometry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2775ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset US QA data\n",
    "US_QA = psql_bboxes[psql_bboxes['cc'] == 'US']\n",
    "print(US_QA['category'].value_counts())\n",
    "\n",
    "f\"Number of confirmed mine features: {len(US_QA['category']=='a')}\"\n",
    "\n",
    "# Write to shapefile to inspect\n",
    "#US_QA.to_file('data/US_QA')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12bbb82",
   "metadata": {},
   "source": [
    "#### Assess distribution of pits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57711a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_global_mines = psql_bboxes[psql_bboxes['category']=='a']\n",
    "\n",
    "QA_global_mines_gdf = gpd.GeoDataFrame(QA_global_mines, geometry = 'geometry', crs = 4326)\n",
    "\n",
    "QA_global_mines_gdf = QA_global_mines_gdf.to_crs(epsg=9822)\n",
    "\n",
    "pit_areas = QA_global_mines_gdf.geometry.area\n",
    "\n",
    "pit_areas.hist(bins=10)\n",
    "\n",
    "print(len(pit_areas))\n",
    "\n",
    "print(pit_areas.min())\n",
    "print(pit_areas.max())\n",
    "print(pit_areas.median())\n",
    "print(pit_areas.mean())\n",
    "\n",
    "print(math.sqrt(pit_areas.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd3f747",
   "metadata": {},
   "outputs": [],
   "source": [
    "pit_areas_majority = pit_areas[pit_areas<(0.008*1e8)]\n",
    "print(len(pit_areas_majority)/len(pit_areas))\n",
    "\n",
    "plt.hist(pit_areas_majority,bins=10)\n",
    "plt.title('Histogram of pit lake areas')\n",
    "#plt.xticks([0, 1, 2], ['Low', 'Medium', 'High'])\n",
    "\n",
    "#plt.show()\n",
    "\n",
    "mode_length = math.sqrt(0.1*1e6)\n",
    "print(mode_length)\n",
    "print(math.sqrt(800000))\n",
    "print(len(pit_areas[pit_areas<(100000)]))\n",
    "\n",
    "import scipy\n",
    "print(np.percentile(pit_areas, 25))\n",
    "print(np.percentile(pit_areas, 75))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e116927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize with satellite basemap\n",
    "providers = cx.providers.flatten()\n",
    "\n",
    "# Visualize mine point data\n",
    "ax = mine_pts.plot(column = 'Material', \n",
    "    markersize = 2,\n",
    "    color = 'blue', \n",
    "    legend = True, \n",
    "    legend_kwds={'bbox_to_anchor': (1, 1)},\n",
    "    figsize = (10,10),\n",
    "    alpha = 0.5)\n",
    "cx.add_basemap(ax, crs=mine_pts.crs, source=providers['NASAGIBS.BlueMarble'])\n",
    "\n",
    "mine_poly.plot(ax=ax, color = 'red', linewidth = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de6b171",
   "metadata": {},
   "source": [
    "#### Filter QA'ed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f725eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = gpd.read_file('data/state_boundaries')\n",
    "\n",
    "# Get only QA'ed pits that are 0.5 or greater (i.e. have been QA'ed and/or higher likelihood)\n",
    "print(f'Number of features in US dataset: {len(US_QA)}')\n",
    "US_QA['score'] = US_QA['score'].apply(lambda score: float(score))\n",
    "US_QA_filtered = US_QA[US_QA['score'] > 0.5]\n",
    "US_QA_filtered['feature_idx'] = US_QA_filtered.reset_index(drop=True).index # Use for dissolve on spatial joining\n",
    "print(f'Number of features in US with > 50% probability: {len(US_QA_filtered)}')\n",
    "\n",
    "\n",
    "# Get only states with pit lakes\n",
    "states['geometry'] = states['geometry'].apply(lambda geom: gpd.GeoSeries([geom]).unary_union)\n",
    "states = gpd.GeoDataFrame(states, geometry='geometry', crs=states.crs).to_crs(epsg = 4326)\n",
    "states = states[~states['STUSPS'].isin(['VI','PR','GU','MP','AS'])]\n",
    "\n",
    "states_with_pits = gpd.sjoin(states, US_QA_filtered, how = 'left', predicate = 'intersects')\n",
    "states_with_pits = states_with_pits[~states_with_pits['score'].isna()]\n",
    "\n",
    "# Take out states with less than 5 pit lakes\n",
    "states_with_enough_pits = states_with_pits['STUSPS'].value_counts() > 5\n",
    "states_list = states_with_enough_pits[states_with_enough_pits].index.to_list()\n",
    "\n",
    "states_with_low_pits = states_with_pits['STUSPS'].value_counts() < 5\n",
    "print(f'States to remove: {states_with_low_pits[states_with_low_pits].index}')\n",
    "\n",
    "states_filtered = states_with_pits[states_with_pits['STUSPS'].isin(states_list)].drop_duplicates('geometry')[['STUSPS','geometry']]\n",
    "\n",
    "states_geometries = states[states['STUSPS'].isin(states_list)]\n",
    "print(f'# of states being used: {len(states_geometries)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67ce6bc",
   "metadata": {},
   "source": [
    "### State by state approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f64379",
   "metadata": {},
   "source": [
    "#### Ohio, Kentucky, West Virginia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efa1df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pit lakes in OH, KY, WV\n",
    "\n",
    "states = gpd.read_file('data/state_boundaries')\n",
    "oh = states[states['STUSPS']=='OH']\n",
    "oh = oh.to_crs(epsg = 4326).geometry.unary_union\n",
    "\n",
    "oh_qa = US_QA[US_QA.geometry.intersects(oh)]\n",
    "print(f'OH QAed mines: {(oh_qa['category'] == 'a').sum()}')\n",
    "\n",
    "ky = states[states['STUSPS']=='KY']\n",
    "ky = ky.to_crs(epsg = 4326).geometry.unary_union\n",
    "\n",
    "ky_qa = US_QA[US_QA.geometry.intersects(ky)]\n",
    "print(f'KY QAed mines: {(ky_qa['category'] == 'a').sum()}')\n",
    "\n",
    "wv = states[states['STUSPS']=='WV']\n",
    "wv = wv.to_crs(epsg = 4326).geometry.unary_union\n",
    "wv_qa = US_QA[US_QA.geometry.intersects(wv)]\n",
    "print(f'WV QAed mines: {(wv_qa['category'] == 'a').sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df2bc43",
   "metadata": {},
   "source": [
    "#### Arizona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b6a702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pit lakes in AZ\n",
    "\n",
    "states = gpd.read_file('data/state_boundaries')\n",
    "az = states[states['STUSPS']=='AZ']\n",
    "az = az.to_crs(epsg = 4326).geometry.unary_union\n",
    "\n",
    "#az_mines_pts = mine_pts[mine_pts.geometry.within(az)]\n",
    "#az_mines_poly = mine_poly[mine_poly.geometry.intersects(az)]\n",
    "#az_mines_intersect = mine_pts_polys[mine_pts_polys.geometry.intersects(az)] # Only keep polygons with confirmed Aquarry points\n",
    "\n",
    "az_qa = US_QA[US_QA.geometry.intersects(az)]\n",
    "print(f'AZ QAed mines: {(az_qa['category'] == 'a').sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd01633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hold out Miami AZ\n",
    "\n",
    "miami_az = gpd.read_file('data/miami_az_pits.geojson')\n",
    "az_mines_intersect = az_mines_intersect[~az_mines_intersect.geometry.within(miami_az.geometry.iloc[0])]\n",
    "\n",
    "print(len(az_mines_intersect))\n",
    "\n",
    "az_qa = az_qa[~az_qa.geometry.intersects(miami_az.geometry.iloc[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e46944",
   "metadata": {},
   "source": [
    "#### Get MN data and separate mines / other bodies of water"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05147f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pit lakes in Minnesota\n",
    "\n",
    "states = gpd.read_file('data/state_boundaries')\n",
    "mn = states[states['STUSPS']=='MN']\n",
    "mn = mn.to_crs(epsg = 4326).geometry.unary_union # MN was multipolygon\n",
    "\n",
    "mn_qa = US_QA[US_QA.geometry.intersects(mn)]\n",
    "print(f'MN QAed mines: {(mn_qa['category'] == 'a').sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9983da89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data source: https://gisdata.mn.gov/dataset/water-dnr-hydrography\n",
    "\n",
    "mn_water_features = gpd.read_file('data/dnr_hydro_features')\n",
    "mn_water_features = mn_water_features[['wb_class','shape_Area','geometry']]\n",
    "mine_classes = ['Natural Ore Mine','Mine Pit Lake','Mine Pit Lake (NF)','Tac/Natural Ore Mine']\n",
    "\n",
    "dnr_pit_lakes = mn_water_features.loc[mn_water_features['wb_class'].isin(mine_classes)].to_crs(epsg=4326)\n",
    "\n",
    "dnr_water = mn_water_features[~mn_water_features['wb_class'].isin(mine_classes)].to_crs(epsg=4326)\n",
    "\n",
    "print(len(dnr_pit_lakes))\n",
    "print(len(dnr_water))\n",
    "\n",
    "dnr_pit_lakes.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3344def",
   "metadata": {},
   "outputs": [],
   "source": [
    "mn_qa_pits = mn_qa[mn_qa['category']=='a']\n",
    "overlap_check = gpd.sjoin(mn_qa_pits, dnr_pit_lakes, how = 'inner', predicate = \"intersects\")\n",
    "overlap_check = overlap_check.drop_duplicates(subset=[\"objectid\"])\n",
    "print(f\"% of mine QA'ed data with overlapping polygon from MN DNR: {len(overlap_check)/len(mn_qa_pits)*100}\")\n",
    "\n",
    "overlap_check = gpd.sjoin(dnr_pit_lakes, mn_qa_pits, how = 'inner', predicate = \"intersects\")\n",
    "overlap_check = overlap_check.drop_duplicates(subset=[\"geometry\"])\n",
    "print(f\"% of DNR polygons with overlapping mine QA'ed data: {len(overlap_check)/len(dnr_pit_lakes)*100}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7305a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize overlap of Minnesota Aquarry QAed mines and DNR polygons\n",
    "providers = cx.providers.flatten()\n",
    "\n",
    "overlap = gpd.sjoin(dnr_pit_lakes,mn_qa_pits,how = \"inner\",rsuffix=\"1\")\n",
    "\n",
    "ax = overlap.plot(edgecolor = 'blue')\n",
    "dnr_pit_lakes.plot(ax = ax,edgecolor = 'red', alpha = 0.5)\n",
    "mn_qa_pits.plot(ax = ax, color = 'green',markersize = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f104046f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hold out Crosby MN\n",
    "\n",
    "crosby_mn = gpd.read_file('data/crosby_mn.geojson')\n",
    "dnr_pit_lakes = dnr_pit_lakes[~dnr_pit_lakes.geometry.intersects(crosby_mn.geometry.iloc[0])]\n",
    "mn_qa = mn_qa[~mn_qa.geometry.intersects(crosby_mn.geometry.iloc[0])]\n",
    "dnr_water = dnr_water[~dnr_water.geometry.intersects(crosby_mn.geometry.iloc[0])]\n",
    "\n",
    "print((mn_qa['category']=='a').sum())\n",
    "print(len(dnr_water))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6115efed",
   "metadata": {},
   "source": [
    "#### Visualize AZ and MN mines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e511e596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize AZ pit lakes\n",
    "providers = cx.providers.flatten()\n",
    "\n",
    "ax = az_mines_intersect.plot()\n",
    "cx.add_basemap(ax, crs=az_mines_intersect.crs, source=providers['NASAGIBS.BlueMarble'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f84323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize MN pit lakes\n",
    "providers = cx.providers.flatten()\n",
    "\n",
    "ax = mn_qa.plot()\n",
    "cx.add_basemap(ax, crs=mn_qa.crs, source=providers['NASAGIBS.BlueMarble'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d84371a",
   "metadata": {},
   "source": [
    "### Check on handling of clouds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7545e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab a cloudy scene \n",
    "# 7/30 is very cloudy, 7/27 has clouds in top left corner\n",
    "\n",
    "cloudy_scene = \"data/MN_v1_5/UL/2024/7/S2B_15TUL_20240730_0_L2A/S2B_15TUL_20240730_0_L2A.parquet\"\n",
    "\n",
    "cloudy_scene_chips = gpd.read_parquet(cloudy_scene)\n",
    "cloudy_scene_chips = gpd.GeoDataFrame(cloudy_scene_chips).set_crs(epsg=4326)\n",
    "\n",
    "print(len(cloudy_scene_chips))\n",
    "\n",
    "# Perform PCA on scene\n",
    "cloudy_pca = EmbeddingsPCA(cloudy_scene_chips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c9058e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize scene PCA\n",
    "\n",
    "cloudy_pca.plot(column='pca1', legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da1666b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the first principal component\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "    # Plot for category 1\n",
    "ax.scatter(cloudy_pca.index,\n",
    "    cloudy_pca[cloudy_pca['clouds'] == True]['pca1'], \n",
    "           color='blue', alpha=0.5)\n",
    "\n",
    "    # Plot for category 2\n",
    "#ax.scatter(cloudy_pca.index,\n",
    "#    cloudy_pca[cloudy_pca['clouds'] == False]['pca1'], \n",
    "#           color='grey', alpha=0.05)\n",
    "    \n",
    "\"\"\" EDIT SO CAN SEE WHAT THE LITTLE OVERLAP IS  plt.scatter(data[data['wb_'] == new_condition]['pca1'], \n",
    "            data[data[new_class_column] == new_condition]['pca2'], \n",
    "            color='red', label=label3, alpha=0.5, marker='^') \"\"\"\n",
    "\n",
    "plt.xlabel('chips')\n",
    "plt.ylabel('PCA 1')\n",
    "ax.legend()\n",
    "plt.title('PCA of Cloudy Scene Embeddings')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4120a4",
   "metadata": {},
   "source": [
    "### Grab embeddings for mines and random embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d19f44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for getting embedding files that intersect with polygons of interest\n",
    "# USE WHEN DON'T HAVE BIGGER GRID ZONES TO SEARCH THROUGH\n",
    "\n",
    "def GetEmbeddingsFromIntersection1_5(folder_path, polygons, random_images = True, count = 35):\n",
    "    \"\"\" Take a folder of embeddings as a string and a set of polygons and return 1) scene file names with intersecting \n",
    "    chips and 2) a random set of non-intersecting image files\n",
    "    \n",
    "    Parameters:\n",
    "    folder_path (str): folder with embedding gpqs\n",
    "    polygons (GeoDataFrame): positive polygon masks \n",
    "    random_images (bool): True if you want random negative images\n",
    "    \n",
    "    Returns:\n",
    "    positive_embeddings, negative embeddings (list, list): one positive, one negative set of scenes  \"\"\"\n",
    "\n",
    "    positive_embeddings = []\n",
    "    negatives_list = []\n",
    "    negative_embeddings = []\n",
    "\n",
    "    # Go through all embedding folders\n",
    "    # Folder structure: Grid zone, year, month, embedding folder, parquet file\n",
    "    grid_zone_folders = [os.path.join(folder_path, folder) for folder in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, folder))]\n",
    "\n",
    "    for grid_zone_folder in grid_zone_folders:\n",
    "        grid_zone = os.path.basename(grid_zone_folder)\n",
    "        # Get list of year folders\n",
    "        year_folders = [os.path.join(grid_zone_folder, year) for year in os.listdir(grid_zone_folder) if os.path.isdir(os.path.join(grid_zone_folder, year))]\n",
    "    \n",
    "        for year_folder in year_folders:\n",
    "            year = os.path.basename(year_folder)\n",
    "            # Get list of month folders\n",
    "            month_folders = [os.path.join(year_folder, month) for month in os.listdir(year_folder) if os.path.isdir(os.path.join(year_folder, month))]\n",
    "            \n",
    "            for month_folder in month_folders:\n",
    "                month = os.path.basename(month_folder)\n",
    "                year_month = f\"{year}_{month}\"\n",
    "                    \n",
    "                # Get list of embedding folders\n",
    "                embedding_folders = [os.path.join(month_folder, embedding) for embedding in os.listdir(month_folder) if os.path.isdir(os.path.join(month_folder, embedding))]\n",
    "                \n",
    "                for embedding_folder in embedding_folders:\n",
    "                    # Get all parquet files in the embedding folder\n",
    "                    parquet_files = [os.path.join(embedding_folder, f) for f in os.listdir(embedding_folder) if f.endswith(\".parquet\")]\n",
    "\n",
    "                    parquet_files_details = []\n",
    "\n",
    "                    # Append parquet file details with grid zone and year_month to the list\n",
    "                    for parquet_file in parquet_files:\n",
    "                        parquet_files_details.append({\n",
    "                            \"file_path\": parquet_file,\n",
    "                            \"grid_zone\": grid_zone,\n",
    "                            \"year_month\": year_month\n",
    "                        })\n",
    "\n",
    "                    # In each folder, go through parquet files and add file to list if it contains mines\n",
    "                    # Check if image has any intersecting chips\n",
    "                    for file in parquet_files_details:\n",
    "                        if os.path.getsize(file['file_path']) > 0:\n",
    "                           pqs = gpd.read_parquet(file['file_path'], columns=[\"geometry\"])\n",
    "                        else:\n",
    "                            print(f\"Skipping empty file: {file['file_path']}\")\n",
    "                        if not gpd.sjoin(pqs, polygons, how=\"inner\", predicate=\"intersects\", rsuffix=\"_1\").empty:\n",
    "                            positive_embeddings.append(file)\n",
    "                        else: \n",
    "                            if random_images == True: # If there is no intersection, choose X number of random images to grab embeddings\n",
    "                                negatives_list.append(file)\n",
    "                                            \n",
    "                    if random_images == True:\n",
    "                        # Set up to get random images\n",
    "                        index = np.array(random.sample(range(1, len(negatives_list)), count))\n",
    "\n",
    "                        for i in index:\n",
    "                            negative_embeddings.append(negatives_list[i])           \n",
    "                        print(f'Random images: {len(negative_embeddings)}')\n",
    "\n",
    "    return positive_embeddings, negative_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "76563af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for getting embedding files that intersect with polygons of interest\n",
    "# USE WHEN HAVE BIGGER GRID ZONES TO SEARCH THROUGH, (i.e. 16T, 16S, etc.)\n",
    "\n",
    "def GetEmbeddingsFromIntersection1_5(folder_path, polygons, random_images = True, count = 35):\n",
    "    \"\"\" Take a folder of embeddings as a string and a set of polygons and return 1) scene file names with intersecting \n",
    "    chips and 2) a random set of non-intersecting image files\n",
    "    \n",
    "    Parameters:\n",
    "    folder_path (str): folder with embedding gpqs\n",
    "    polygons (GeoDataFrame): positive polygon masks \n",
    "    random_images (bool): True if you want random negative images\n",
    "    \n",
    "    Returns:\n",
    "    positive_embeddings, negative embeddings (list, list): one positive, one negative set of scenes  \"\"\"\n",
    "\n",
    "    positive_embeddings = []\n",
    "    negatives_list = []\n",
    "    negative_embeddings = []\n",
    "\n",
    "    # Go through all embedding folders\n",
    "    # Folder structure: latitude grid, Grid zone, year, month, embedding folder, parquet file\n",
    "    latitude_grid_folders = [os.path.join(folder_path, folder) for folder in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, folder))]\n",
    "\n",
    "    for latitude_grid_folder in latitude_grid_folders:\n",
    "        latitude_grid = os.path.basename(latitude_grid_folder)\n",
    "        # Get list of grid zone folders\n",
    "        grid_zone_folders = [os.path.join(latitude_grid_folder, folder) for folder in os.listdir(latitude_grid_folder) if os.path.isdir(os.path.join(latitude_grid_folder, folder))]\n",
    "\n",
    "        for grid_zone_folder in grid_zone_folders:\n",
    "            grid_zone = os.path.basename(grid_zone_folder)\n",
    "            # Get list of year folders\n",
    "            year_folders = [os.path.join(grid_zone_folder, year) for year in os.listdir(grid_zone_folder) if os.path.isdir(os.path.join(grid_zone_folder, year))]\n",
    "    \n",
    "            for year_folder in year_folders:\n",
    "                year = os.path.basename(year_folder)\n",
    "                # Get list of month folders\n",
    "                month_folders = [os.path.join(year_folder, month) for month in os.listdir(year_folder) if os.path.isdir(os.path.join(year_folder, month))]\n",
    "                \n",
    "                for month_folder in month_folders:\n",
    "                    month = os.path.basename(month_folder)\n",
    "                    year_month = f\"{year}_{month}\"\n",
    "                        \n",
    "                    # Get list of embedding folders\n",
    "                    embedding_folders = [os.path.join(month_folder, embedding) for embedding in os.listdir(month_folder) if os.path.isdir(os.path.join(month_folder, embedding))]\n",
    "                    \n",
    "                    for embedding_folder in embedding_folders:\n",
    "                        # Get all parquet files in the embedding folder\n",
    "                        parquet_files = [os.path.join(embedding_folder, f) for f in os.listdir(embedding_folder) if f.endswith(\".parquet\")]\n",
    "\n",
    "                        parquet_files_details = []\n",
    "\n",
    "                        # Append parquet file details with grid zone and year_month to the list\n",
    "                        for parquet_file in parquet_files:\n",
    "                            parquet_files_details.append({\n",
    "                                \"file_path\": parquet_file,\n",
    "                                \"grid_zone\": grid_zone,\n",
    "                                \"year_month\": year_month\n",
    "                            })\n",
    "\n",
    "                        # In each folder, go through parquet files and add file to list if it contains mines\n",
    "                        # Check if image has any intersecting chips\n",
    "                        for file in parquet_files_details:\n",
    "                            if os.path.getsize(file['file_path']) > 0:\n",
    "                                pqs = gpd.read_parquet(file['file_path'], columns=[\"geometry\"])\n",
    "                            else:\n",
    "                                print(f\"Skipping empty file: {file['file_path']}\")\n",
    "                            if not gpd.sjoin(pqs, polygons, how=\"inner\", predicate=\"intersects\", rsuffix=\"_1\").empty:\n",
    "                                positive_embeddings.append(file)\n",
    "                            else: \n",
    "                                if random_images == True: # If there is no intersection, choose X number of random images to grab embeddings\n",
    "                                    negatives_list.append(file)\n",
    "                                                \n",
    "                        if random_images == True:\n",
    "                            # Set up to get random images\n",
    "                            index = np.array(random.sample(range(1, len(negatives_list)), count))\n",
    "\n",
    "                            for i in index:\n",
    "                                negative_embeddings.append(negatives_list[i])           \n",
    "                            print(f'Random images: {len(negative_embeddings)}')\n",
    "\n",
    "    return positive_embeddings, negative_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "790b426c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get monthly aggregates of chip embeddings for a given set of embedded images (not averaging)\n",
    "# Use if chip geometries overlap substantially\n",
    "\n",
    "def MonthlyComposite(embeddings_df, polygons = None):\n",
    "    \"\"\" Take a dataframe of embedding geoparquet files and get corresponding chips with < 25% overlap\n",
    "\n",
    "    Parameters:\n",
    "    embeddings_df (DataFrame): dataframe with three columns, embeddings file paths, \n",
    "    polygons (GeoDataFrame): gdf containing geometries of features of intersect. If specified, returns only chips\n",
    "    that intersect these\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    monthly_agg_embeddings = gpd.GeoDataFrame()\n",
    "    if polygons is not None:\n",
    "        polys_geometry = polygons.unary_union\n",
    "\n",
    "    for zone in embeddings_df['grid_zone'].unique(): # For each UTM zone\n",
    "        zone_embeddings = pd.DataFrame(embeddings_df[embeddings_df['grid_zone'] == str(zone)])\n",
    "        count = 1\n",
    "\n",
    "        for index, row in zone_embeddings.iterrows(): # For each date within UTM zone\n",
    "             # Will need to adjust when more months are available, currently just goes through whole list\n",
    "            chip_embeddings_agg_gdf = gpd.GeoDataFrame()\n",
    "            non_intersecting_chips = gpd.GeoDataFrame()\n",
    "            chip_embeddings = gpd.read_parquet(row['file_path'])\n",
    "            chip_embeddings = gpd.GeoDataFrame(chip_embeddings).set_crs(epsg=4326)\n",
    "\n",
    "            if len(chip_embeddings) > 2000: # If the image is not very cloudy\n",
    "                if polygons is not None: # Don't include chips that don't correspond to the polygons\n",
    "                    chip_embeddings = chip_embeddings[chip_embeddings.geometry.intersects(polys_geometry)]\n",
    "                if not monthly_agg_embeddings.empty:\n",
    "                    # Get all chips that aren't already in the aggregation\n",
    "                    chips_intersection = chip_embeddings.geometry.intersects(monthly_agg_embeddings_unified)\n",
    "                    non_intersecting_chips = chip_embeddings[~chips_intersection]\n",
    "                    chip_embeddings_agg_gdf = non_intersecting_chips.rename(columns={'embeddings_left': 'embeddings'})\n",
    "                    \n",
    "                    # Add chips that intersect with aggregation that are less than 25% overlapping\n",
    "                    intersecting_chips = chip_embeddings[chips_intersection]\n",
    "                    intersecting_chips['chip_area'] = intersecting_chips.geometry.area\n",
    "                    intersections = intersecting_chips.geometry.intersection(monthly_agg_embeddings_unified)\n",
    "                    intersecting_chips['area_overlap'] = intersections.area\n",
    "                    low_intersection_mask = (intersecting_chips['area_overlap'] / intersecting_chips['chip_area']) < 0.25\n",
    "                    low_intersection_chips = intersecting_chips[low_intersection_mask].rename(columns={'embeddings_left': 'embeddings'})\n",
    "                    # Add chips with small overlap to aggregated embeddings\n",
    "                    chip_embeddings_agg_gdf = pd.concat([chip_embeddings_agg_gdf,low_intersection_chips])\n",
    "                else:\n",
    "                    non_intersecting_chips = chip_embeddings\n",
    "                    chip_embeddings_agg_gdf = gpd.GeoDataFrame(non_intersecting_chips['embeddings'], columns = ['embeddings'], geometry=non_intersecting_chips['geometry'])\n",
    "                chip_embeddings_agg_gdf['grid_zone'] = zone\n",
    "                chip_embeddings_agg_gdf['year_month'] = zone_embeddings['year_month'].iloc[0]\n",
    "\n",
    "                # Add additional chips\n",
    "                monthly_agg_embeddings = pd.concat([monthly_agg_embeddings, chip_embeddings_agg_gdf])\n",
    "\n",
    "                # Get geometries of aggregated chips to compare against\n",
    "                chip_embeddings_agg_gdf_unified = chip_embeddings_agg_gdf.unary_union\n",
    "                monthly_agg_embeddings_unified = monthly_agg_embeddings.union(chip_embeddings_agg_gdf_unified).unary_union\n",
    "                #count = count + 1\n",
    "                print('added') \n",
    "                #if count == 3: # Check two images without clouds\n",
    "                    #count = 1\n",
    "                break\n",
    "    \n",
    "    monthly_agg_embeddings.to_crs(epsg=4326)\n",
    "    monthly_agg_embeddings = monthly_agg_embeddings.drop_duplicates('geometry')\n",
    "\n",
    "    return monthly_agg_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b13e985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings by region\n",
    "def RegionalData(region, zone, negatives = True):\n",
    "    qa_pits = gpd.sjoin(US_QA_filtered, region, how = 'inner', predicate = 'intersects')\n",
    "    qa_pits = qa_pits.dissolve(by = 'feature_idx')\n",
    "\n",
    "    us_qa_pits = qa_pits[qa_pits['category'] == 'a']\n",
    "    us_qa_nonlakes = qa_pits[~qa_pits['category'].isin(['a','q','\\\\N'])]\n",
    "\n",
    "    print(f'Number of features identified as pit lakes in overlapping states in the region: {len(us_qa_pits)}')\n",
    "    # Note: this number will exceed the actual amount in the region\n",
    "\n",
    "    # Get embeddings for pit lakes and no pit lakes present in US zone from QA data\n",
    "    print(f'data/US_v1_5/{zone}')\n",
    "    mine_embeddings_us, non_mine_embeddings_us = GetEmbeddingsFromIntersection1_5(folder_path = f'data/US_v1_5/{zone}', polygons = us_qa_pits, random_images=False)\n",
    "    print(f\"Number of images with mines: {len(mine_embeddings_us)}\")\n",
    "\n",
    "    # Get monthly embeddings for pit lake embeddings in MN\n",
    "    mine_embeddings_us_df = pd.DataFrame(mine_embeddings_us, columns = ['file_path','grid_zone','year_month'])\n",
    "\n",
    "    us_positive_embs_avg = MonthlyComposite(mine_embeddings_us_df, us_qa_pits)\n",
    "    print(f'Number of positive chips: {len(us_positive_embs_avg)}')\n",
    "    \n",
    "    us_negative_embs_avg = gpd.GeoDataFrame()\n",
    "    if negatives == True:\n",
    "        # Get embeddings for features identified as not lakes from QA\n",
    "        qa_non_mine_embeddings_us, dummy = GetEmbeddingsFromIntersection1_5(folder_path = f'data/US_v1_5/{zone}', polygons = us_qa_nonlakes, random_images=False)\n",
    "        print(f\"Number of negative mine images: {len(qa_non_mine_embeddings_us)}\")\n",
    "\n",
    "        # Get monthly embeddings for water and non-pit lake embeddings in MN\n",
    "        qa_non_mine_embeddings_us_df = pd.DataFrame(qa_non_mine_embeddings_us, columns = ['file_path','grid_zone','year_month'])\n",
    "\n",
    "        us_negative_embs_avg = MonthlyComposite(qa_non_mine_embeddings_us_df, us_qa_nonlakes)\n",
    "        print(f'Number of negative chips: {len(us_negative_embs_avg)}')\n",
    "\n",
    "    return us_positive_embs_avg, us_negative_embs_avg\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc767de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get geometries for region 16 and 17\n",
    "region16 = states_filtered[states_filtered['STUSPS'].isin(['MN', 'MI', 'IL', 'IN', 'OH', 'MS', 'AL', 'GA', 'LA', 'FL', 'MO'])]\n",
    "\n",
    "qa_pits_16 = gpd.sjoin(US_QA_filtered, region16, how = 'inner', predicate = 'intersects')\n",
    "qa_pits_16['geometry'] = qa_pits_16.geometry\n",
    "qa_pits_16 = qa_pits_16.dissolve(by = 'feature_idx')\n",
    "\n",
    "us_qa_pits_16 = qa_pits_16[qa_pits_16['category'] == 'a']\n",
    "us_qa_nonlakes_16 = qa_pits_16[~qa_pits_16['category'].isin(['a','q','\\\\N'])]\n",
    "\n",
    "print(len(us_qa_pits_16))\n",
    "\n",
    "region17 = states_filtered[states_filtered['STUSPS'].isin(['MI', 'OH', 'NY', 'PA', 'WV', 'VA', 'NC', 'SC', 'KY', 'TN', 'GA','FL'])]\n",
    "\n",
    "qa_pits_17 = gpd.sjoin(US_QA_filtered, region17, how = 'inner', predicate = 'intersects')\n",
    "qa_pits_17['geometry'] = qa_pits_17.geometry\n",
    "qa_pits_17 = qa_pits_17.dissolve(by = 'feature_idx')\n",
    "\n",
    "us_qa_pits_17 = qa_pits_17[qa_pits_17['category'] == 'a']\n",
    "us_qa_nonlakes_17 = qa_pits_17[~qa_pits_17['category'].isin(['a','q','\\\\N'])]\n",
    "\n",
    "print(len(us_qa_pits_17))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "885c2222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "111\n",
      "Number of negative mine images: 711\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "62\n",
      "528\n",
      "data/US_v1_5/16/R\n",
      "Number of images with mines: 179\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "29\n",
      "Number of negative mine images: 154\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "17\n",
      "656\n",
      "data/US_v1_5/16/S\n",
      "Number of images with mines: 636\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "152\n",
      "Number of negative mine images: 563\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "77\n"
     ]
    }
   ],
   "source": [
    "# Get grid region geometries and get corresponding positive and negative embeddings from QA dataset\n",
    "\n",
    "region16T = states_filtered[states_filtered['STUSPS'].isin(['MN', 'MI', 'IL', 'IN', 'OH'])]\n",
    "region16S = states_filtered[states_filtered['STUSPS'].isin(['IL', 'IN', 'OH', 'MS', 'AL', 'GA', 'MO'])]\n",
    "region16R = states_filtered[states_filtered['STUSPS'].isin(['MS', 'AL', 'GA','LA','FL'])]\n",
    "\n",
    "us16T_positive_embs_avg, us16T_negative_embs_avg = RegionalData(region16T, '16/T')\n",
    "us16R_positive_embs_avg, us16R_negative_embs_avg = RegionalData(region16R, '16/R')\n",
    "us16S_positive_embs_avg, us16S_negative_embs_avg = RegionalData(region16S, '16/S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e28eb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "573\n",
      "data/US_v1_5/17/S\n",
      "Number of images with mines: 494\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "146\n",
      "Number of negative mine images: 462\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "122\n",
      "123\n",
      "data/US_v1_5/17/T\n",
      "Skipping empty file: data/US_v1_5/17/T\\MG\\2024\\9\\S2B_17TMG_20240907_0_L2A\\S2B_17TMG_20240907_0_L2A.parquet\n",
      "Number of images with mines: 466\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "83\n",
      "Skipping empty file: data/US_v1_5/17/T\\MG\\2024\\9\\S2B_17TMG_20240907_0_L2A\\S2B_17TMG_20240907_0_L2A.parquet\n",
      "Number of negative mine images: 445\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "added\n",
      "69\n",
      "412\n",
      "data/US_v1_5/17/R\n",
      "Number of images with mines: 159\n",
      "added\n",
      "added\n"
     ]
    }
   ],
   "source": [
    "# Get grid region geometries and get corresponding positive and negative embeddings from QA dataset\n",
    "\n",
    "region17T = states_filtered[states_filtered['STUSPS'].isin(['MI', 'OH', 'PA','NY'])]\n",
    "region17S = states_filtered[states_filtered['STUSPS'].isin(['OH', 'PA', 'WV', 'VA', 'NC', 'SC', 'KY', 'TN', 'GA'])]\n",
    "region17R = states_filtered[states_filtered['STUSPS'].isin(['GA','FL'])]\n",
    "\n",
    "us17S_positive_embs_avg, us17S_negative_embs_avg = RegionalData(region17S, '17/S')\n",
    "us17T_positive_embs_avg, us17T_negative_embs_avg = RegionalData(region17T, '17/T')\n",
    "us17R_positive_embs_avg, us17R_negative_embs_avg = RegionalData(region17R, '17/R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "bf3120c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate into UTM vertical zones\n",
    "\n",
    "us16_positive_embs_avg = pd.concat([us16R_positive_embs_avg, us16S_positive_embs_avg, us16T_positive_embs_avg])\n",
    "us16_negative_embs_avg = pd.concat([us16R_negative_embs_avg, us16S_negative_embs_avg, us16T_negative_embs_avg])\n",
    "\n",
    "us17_positive_embs_avg = pd.concat([us17R_positive_embs_avg, us17S_positive_embs_avg, us17T_positive_embs_avg])\n",
    "us17_negative_embs_avg = pd.concat([us17R_negative_embs_avg, us17S_negative_embs_avg, us17T_negative_embs_avg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca929e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize pit lake chips\n",
    "fig, ax = plt.subplots()\n",
    "us16_positive_embs_avg['geometry'].plot(ax = ax, facecolor = \"none\", edgecolor = \"blue\")\n",
    "us17_positive_embs_avg['geometry'].plot(ax = ax, facecolor = \"none\", edgecolor = \"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8ab96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD MINNESOTA DNR DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37d1961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Aquarry QA and DNR data to check for intersection\n",
    "\n",
    "mn_qa_pits = mn_qa[mn_qa['category'] == 'a'] # QAed pit lakes \n",
    "\n",
    "mn_aggregated_pits = gpd.overlay(mn_qa_pits, dnr_pit_lakes, how=\"union\")\n",
    "\n",
    "# Get embedding files for pit lakes in MN in DNR or Aquarry dataset\n",
    "mine_embeddings_mn, dummy = GetEmbeddingsFromIntersection1_5(folder_path = 'data/MN_v1_5/', polygons = mn_aggregated_pits, random_images = False)\n",
    "print(f\"Number of images with mines: {len(mine_embeddings_mn)}\")\n",
    "\n",
    "mn_qa_nonlakes = mn_qa[~mn_qa['category'].isin(['a','q','\\\\N'])] # QAed other mining features, without questionable features \n",
    "\n",
    "# Get embeddings for lakes/water in MN\n",
    "dnr_water = dnr_water.to_crs(epsg=4326)\n",
    "\n",
    "# Reduce sample size\n",
    "random_idx = random.sample(range(1, len(dnr_water)), 1000)\n",
    "dnr_water_cropped = dnr_water.iloc[random_idx]\n",
    "\n",
    "#dnr_water_cropped = dnr_water_cropped[~dnr_water_cropped.geometry.intersects(crosby_mn.geometry.iloc[0])] # Remove any Crosby lakes\n",
    "\n",
    "lake_embeddings_mn, dummy = GetEmbeddingsFromIntersection1_5(folder_path = 'data/MN_v1_5/', polygons = dnr_water_cropped, random_images = False)\n",
    "print(f\"Number of images with water that's not mines: {len(lake_embeddings_mn)}\")\n",
    "\n",
    "\n",
    "# Get embeddings for features identified as not lakes from QA\n",
    "qa_non_mine_embeddings_mn, dummy = GetEmbeddingsFromIntersection1_5(folder_path = 'data/MN_v1_5/', polygons = mn_qa_nonlakes, random_images = False)\n",
    "print(f\"Number of negative mine images: {len(qa_non_mine_embeddings_mn)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "id": "5d2e6307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPRECATED\n",
    "# Make monthly aggregates\n",
    "# USE IF GEOMETRIES DON'T OVERLAP TOO MUCH\n",
    "\n",
    "def MonthlyComposite(embeddings_df):\n",
    "\n",
    "    monthly_average_embeddings = gpd.GeoDataFrame()\n",
    "\n",
    "    for zone in embeddings_df['grid_zone'].unique():\n",
    "        zone_embeddings = pd.DataFrame(embeddings_df[embeddings_df['grid_zone'] == str(zone)])\n",
    "\n",
    "        month_aggregate = gpd.GeoDataFrame()\n",
    "        \n",
    "        for index, row in zone_embeddings.iterrows():\n",
    "             # Will need to adjust when more months are available\n",
    "            chip_embeddings = gpd.read_parquet(row['file_path'])\n",
    "            chip_embeddings = gpd.GeoDataFrame(chip_embeddings).set_crs(epsg=4326)\n",
    "            month_aggregate = pd.concat([month_aggregate,chip_embeddings])\n",
    "\n",
    "            grouped = month_aggregate.groupby('geometry')\n",
    "\n",
    "            chip_embeddings_agg = grouped['embeddings'].apply(list).reset_index()\n",
    "            chip_embeddings_agg_gdf = gpd.GeoDataFrame(chip_embeddings_agg, geometry='geometry')\n",
    "\n",
    "            chip_embeddings_agg_gdf['monthly_average'] = chip_embeddings_agg_gdf['embeddings'].apply(\n",
    "                lambda x: np.mean(np.vstack(x), axis=0))\n",
    "\n",
    "            chip_embeddings_agg_gdf['grid_zone'] = zone\n",
    "            chip_embeddings_agg_gdf['year_month'] = zone_embeddings['year_month'].iloc[0]\n",
    "\n",
    "            monthly_average_embeddings = pd.concat([monthly_average_embeddings, chip_embeddings_agg_gdf])\n",
    "\n",
    "    return monthly_average_embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fbce31",
   "metadata": {},
   "source": [
    "### State by state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccd26f1",
   "metadata": {},
   "source": [
    "#### Arizona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2a6737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings for pit lakes and no pit lakes present in AZ\n",
    "\n",
    "mine_embeddings_az, non_mine_embeddings_az = GetEmbeddingsFromIntersection1_5(folder_path = 'data/AZ_v1_5/', polygons = az_mines_intersect)\n",
    "\n",
    "print(\"Number of images with mines:\")\n",
    "print(len(mine_embeddings_az))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec7d0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average of monthly embeddings for pit lake embeddings in AZ\n",
    "mine_embeddings_az_df = pd.DataFrame(mine_embeddings_az, columns = ['file_path','grid_zone','year_month'])\n",
    "\n",
    "az_positive_embs_avg = MonthlyComposite(mine_embeddings_az_df)\n",
    "print(len(az_positive_embs_avg))\n",
    "\n",
    "# Get average of monthly embeddings for lake and features that aren't lakes in AZ\n",
    "lake_embeddings_az_df = pd.DataFrame(lake_embeddings_az, columns = ['file_path','grid_zone','year_month'])\n",
    "qa_non_mine_embeddings_az_df = pd.DataFrame(qa_non_mine_embeddings_az, columns = ['file_path','grid_zone','year_month'])\n",
    "\n",
    "az_water_embs_avg = MonthlyComposite(lake_embeddings_az_df)\n",
    "az_negative_embs_avg = MonthlyComposite(qa_non_mine_embeddings_az_df)\n",
    "print(len(az_water_embs_avg))\n",
    "print(len(az_negative_embs_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1c415c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check overlap\n",
    "fig, ax = plt.subplots()\n",
    "az_positive_embs_avg['geometry'].plot(ax = ax, facecolor = \"none\", edgecolor = \"blue\")\n",
    "\n",
    "#az_negative_embs_avg['geometry'].plot(ax = ax, facecolor = \"none\", edgecolor = \"blue\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5dece1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings for pit lakes and no pit lakes present in AZ from QA data\n",
    "\n",
    "az_qa_pits = az_qa[az_qa['category'] == 'a'] # QAed pit lakes \n",
    "az_qa_nonlakes = az_qa[~az_qa['category'].isin(['a','q','\\\\N'])] # QAed other mining features, without questionable features \n",
    "\n",
    "mine_embeddings_az, non_mine_embeddings_az = GetEmbeddingsFromIntersection1_5(folder_path = 'data/AZ_v1_5/', polygons = az_qa_pits, random_images=False)\n",
    "print(f\"Number of images with mines: {len(mine_embeddings_az)}\")\n",
    "\n",
    "\n",
    "# Get embeddings for features identified as not lakes from QA\n",
    "qa_non_mine_embeddings_az, dummy = GetEmbeddingsFromIntersection1_5(folder_path = 'data/AZ_v1_5/', polygons = az_qa_nonlakes, random_images=False)\n",
    "print(f\"Number of negative mine images: {len(qa_non_mine_embeddings_az)}\")\n",
    "\n",
    "\n",
    "# Get embeddings for water bodies in AZ\n",
    "az_lakes = gpd.read_file('data/az_lakes.txt')\n",
    "lake_embeddings_az, dummy = GetEmbeddingsFromIntersection1_5(folder_path = 'data/AZ_v1_5/', polygons = az_lakes, random_images=False)\n",
    "\n",
    "print(f\"Number of images with lakes: {len(lake_embeddings_az)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9e3d05",
   "metadata": {},
   "source": [
    "#### Indiana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837865e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pit lakes in IN\n",
    "\n",
    "states = gpd.read_file('data/state_boundaries')\n",
    "indy = states[states['STUSPS']=='IN']\n",
    "indy = indy.to_crs(epsg = 4326).geometry.unary_union\n",
    "\n",
    "in_qa = US_QA[US_QA.geometry.intersects(indy)]\n",
    "print(f'IN QAed mines: {(in_qa['category'] == 'a').sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e70b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings for pit lakes and no pit lakes present in IN from QA data\n",
    "\n",
    "in_qa_pits = in_qa[in_qa['category'] == 'a'] # QAed pit lakes \n",
    "in_qa_nonlakes = in_qa[~in_qa['category'].isin(['a','q','\\\\N'])] # QAed other mining features, without questionable features \n",
    "\n",
    "mine_embeddings_in, non_mine_embeddings_in = GetEmbeddingsFromIntersection1_5(folder_path = 'data/IN_v1_5/', polygons = in_qa_pits, random_images=False)\n",
    "print(f\"Number of images with mines: {len(mine_embeddings_in)}\")\n",
    "\n",
    "\n",
    "# Get embeddings for features identified as not lakes from QA\n",
    "qa_non_mine_embeddings_in, dummy = GetEmbeddingsFromIntersection1_5(folder_path = 'data/IN_v1_5/', polygons = in_qa_nonlakes, random_images=False)\n",
    "print(f\"Number of negative mine images: {len(qa_non_mine_embeddings_in)}\")\n",
    "\n",
    "# Get embeddings for water bodies in IN\n",
    "#az_lakes = gpd.read_file('data/az_lakes.txt')\n",
    "#lake_embeddings_az, dummy = GetEmbeddingsFromIntersection1_5(folder_path = 'data/IN_v1_5/', polygons = az_lakes, random_images=False)\n",
    "\n",
    "#print(f\"Number of images with lakes: {len(lake_embeddings_az)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c253808b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average of monthly embeddings for pit lake embeddings in IN\n",
    "mine_embeddings_in_df = pd.DataFrame(mine_embeddings_in, columns = ['file_path','grid_zone','year_month'])\n",
    "\n",
    "in_positive_embs_avg = MonthlyComposite(mine_embeddings_in_df)\n",
    "print(len(in_positive_embs_avg))\n",
    "\n",
    "# Get average of monthly embeddings for lake and features that aren't lakes in AZ\n",
    "#lake_embeddings_in_df = pd.DataFrame(lake_embeddings_in, columns = ['file_path','grid_zone','year_month'])\n",
    "qa_non_mine_embeddings_in_df = pd.DataFrame(qa_non_mine_embeddings_in, columns = ['file_path','grid_zone','year_month'])\n",
    "\n",
    "#az_water_embs_avg = MonthlyComposite(lake_embeddings_in_df)\n",
    "in_negative_embs_avg = MonthlyComposite(qa_non_mine_embeddings_in_df)\n",
    "#print(len(in_water_embs_avg))\n",
    "print(len(in_negative_embs_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6916f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check mines chips\n",
    "fig, ax = plt.subplots()\n",
    "in_positive_embs_avg['geometry'].plot(ax = ax, facecolor = \"none\", edgecolor = \"blue\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f1ccc2",
   "metadata": {},
   "source": [
    "#### Kentucky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907b9ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings for pit lakes and no pit lakes present in IN from QA data\n",
    "\n",
    "ky_qa_pits = ky_qa[ky_qa['category'] == 'a'] # QAed pit lakes \n",
    "ky_qa_nonlakes = ky_qa[~ky_qa['category'].isin(['a','q','\\\\N'])] # QAed other mining features, without questionable features \n",
    "\n",
    "mine_embeddings_ky, non_mine_embeddings_in = GetEmbeddingsFromIntersection1_5(folder_path = 'data/KY_v1_5/', polygons = ky_qa_pits, random_images=False)\n",
    "print(f\"Number of images with mines: {len(mine_embeddings_ky)}\")\n",
    "\n",
    "# Get embeddings for features identified as not lakes from QA\n",
    "qa_non_mine_embeddings_ky, dummy = GetEmbeddingsFromIntersection1_5(folder_path = 'data/KY_v1_5/', polygons = ky_qa_nonlakes, random_images=False)\n",
    "print(f\"Number of negative mine images: {len(qa_non_mine_embeddings_ky)}\")\n",
    "\n",
    "# Get embeddings for water bodies in IN\n",
    "#az_lakes = gpd.read_file('data/az_lakes.txt')\n",
    "#lake_embeddings_az, dummy = GetEmbeddingsFromIntersection1_5(folder_path = 'data/IN_v1_5/', polygons = az_lakes, random_images=False)\n",
    "\n",
    "#print(f\"Number of images with lakes: {len(lake_embeddings_az)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8935874c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average of monthly embeddings for pit lake embeddings in IN\n",
    "mine_embeddings_ky_df = pd.DataFrame(mine_embeddings_ky, columns = ['file_path','grid_zone','year_month'])\n",
    "\n",
    "ky_positive_embs_avg = MonthlyComposite(mine_embeddings_ky_df)\n",
    "print(len(ky_positive_embs_avg))\n",
    "\n",
    "# Get average of monthly embeddings for lake and features that aren't lakes in AZ\n",
    "#lake_embeddings_in_df = pd.DataFrame(lake_embeddings_in, columns = ['file_path','grid_zone','year_month'])\n",
    "qa_non_mine_embeddings_ky_df = pd.DataFrame(qa_non_mine_embeddings_ky, columns = ['file_path','grid_zone','year_month'])\n",
    "\n",
    "#az_water_embs_avg = MonthlyComposite(lake_embeddings_in_df)\n",
    "ky_negative_embs_avg = MonthlyComposite(qa_non_mine_embeddings_ky_df)\n",
    "#print(len(in_water_embs_avg))\n",
    "print(len(ky_negative_embs_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56768c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check mines chips\n",
    "fig, ax = plt.subplots()\n",
    "ky_positive_embs_avg['geometry'].plot(ax = ax, facecolor = \"none\", edgecolor = \"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8cbc50",
   "metadata": {},
   "source": [
    "#### Minnesota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2df17a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get monthly embeddings for pit lake embeddings in MN\n",
    "mine_embeddings_mn_df = pd.DataFrame(mine_embeddings_mn, columns = ['file_path','grid_zone','year_month'])\n",
    "\n",
    "mn_positive_embs_avg = MonthlyComposite(mine_embeddings_mn_df, mn_aggregated_pits)\n",
    "print(len(mn_positive_embs_avg))\n",
    "\n",
    "# Get monthly embeddings for water and non-pit lake embeddings in MN\n",
    "lake_embeddings_mn_df = pd.DataFrame(lake_embeddings_mn, columns = ['file_path','grid_zone','year_month'])\n",
    "qa_non_mine_embeddings_mn_df = pd.DataFrame(qa_non_mine_embeddings_mn, columns = ['file_path','grid_zone','year_month'])\n",
    "\n",
    "mn_water_embs_avg = MonthlyComposite(lake_embeddings_mn_df, dnr_water_cropped)\n",
    "mn_negative_embs_avg = MonthlyComposite(qa_non_mine_embeddings_mn_df, mn_qa_nonlakes)\n",
    "print(len(mn_water_embs_avg))\n",
    "print(len(mn_negative_embs_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a80a357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e8abea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52b6e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check overlap\n",
    "fig, ax = plt.subplots()\n",
    "mn_positive_embs_avg['geometry'].plot(ax = ax, facecolor = \"none\", edgecolor = \"blue\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84fcd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings for pit lakes in MN in DNR dataset\n",
    "mine_embeddings_mn, dummy = GetEmbeddingsFromIntersection(folder_path = 'data/60cm_mn/rgbir_cog/mn/', polygons = dnr_pit_lakes, random_images = False)\n",
    "\n",
    "print(f\"Number of images with mines: {len(mine_embeddings_mn)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca47c788",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0349b131",
   "metadata": {},
   "source": [
    "### Label embeddings with mine/no mine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 929,
   "id": "4775feb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function that marks each chip in intersecting and non-intersecting set of images as True if intersects chip embedding, False if not\n",
    "\n",
    "def LabelEmbeddings(positive_embeddings, negative_embeddings, polygons = gpd.GeoDataFrame(), polygons2 = gpd.GeoDataFrame()):\n",
    "    \"\"\" Create GeoDataFrame that takes image files with intersection with polygon of interest and returns only positive chips and marks True and \n",
    "    takes images without intersection and mark all chips as False \n",
    "    \n",
    "    Helpful if there may be false negatives within the image with intersecting polygons\n",
    "    \n",
    "    Parameters:\n",
    "    positive_embeddings (GeoDataFrame): positive mask files\n",
    "    negative_embeddings (GeoDataFrame): negative mask files\n",
    "\n",
    "    Returns:\n",
    "    data (GeoDataFrame): chips with positive and negative labels\n",
    "\n",
    "    \"\"\"\n",
    "    data = gpd.GeoDataFrame()\n",
    "\n",
    "    for emb in positive_embeddings:\n",
    "        chip_embeddings = gpd.read_parquet(emb)\n",
    "        chip_embeddings = gpd.GeoDataFrame(chip_embeddings).set_crs(epsg=4326)\n",
    "        positive_chips = gpd.sjoin(chip_embeddings, polygons, predicate='intersects',how = 'inner',rsuffix=\"_1\")\n",
    "        data = pd.concat([data, positive_chips], ignore_index=True)\n",
    "\n",
    "    data['mine'] = 1 # Assign 1 to mine column for chips with embeddings\n",
    "    print(\"Number of positive samples:\") \n",
    "    print(len(data))\n",
    "\n",
    "    if polygons2.empty:\n",
    "        for emb in negative_embeddings:\n",
    "            chip_embeddings = gpd.read_parquet(emb)\n",
    "            chip_embeddings = gpd.GeoDataFrame(chip_embeddings).set_crs(epsg=4326)\n",
    "            chip_embeddings['mine'] = 0 # Assign 0 to mine column for chips without embeddings\n",
    "            data = pd.concat([data, chip_embeddings], ignore_index=True)\n",
    "    else:\n",
    "        for emb in negative_embeddings:\n",
    "            chip_embeddings = gpd.read_parquet(emb)\n",
    "            chip_embeddings = gpd.GeoDataFrame(chip_embeddings).set_crs(epsg=4326)\n",
    "            negative_chips = gpd.sjoin(chip_embeddings, polygons2, predicate='intersects',how = 'inner',rsuffix=\"_1\")\n",
    "            negative_chips['mine'] = 0\n",
    "            data = pd.concat([data, negative_chips], ignore_index=True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "eb7d097e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function that marks each chip in intersecting and non-intersecting set of images as True if intersects chip embedding, False if not\n",
    "\n",
    "def LabelEmbeddings1_5(positive_embeddings, negative_embeddings, polygons = gpd.GeoDataFrame(), polygons2 = gpd.GeoDataFrame()):\n",
    "    \"\"\" Create GeoDataFrame that takes image files with intersection with polygon of interest and returns only positive chips and marks True and \n",
    "    takes images without intersection and mark all chips as False or returns only chips that intersect with polygons2\n",
    "    \n",
    "    Helpful if there may be false negatives within the image with intersecting polygons\n",
    "    \n",
    "    Parameters:\n",
    "    positive_embeddings (GeoDataFrame): positive chip embeddings\n",
    "    negative_embeddings (GeoDataFrame): negative chip embeddings\n",
    "\n",
    "    Returns:\n",
    "    data (GeoDataFrame): chips with positive and negative labels\n",
    "\n",
    "    \"\"\"\n",
    "    data = gpd.GeoDataFrame()\n",
    "\n",
    "    positive_chips = gpd.sjoin(positive_embeddings, polygons, predicate='intersects',how = 'inner',rsuffix=\"_1\")\n",
    "    positive_chips = positive_chips[~positive_chips['index_right'].isna()]\n",
    "    print(len(positive_chips))\n",
    "\n",
    "    data = pd.concat([data, positive_chips], ignore_index=True)\n",
    "    data['mine'] = 1 # Assign 1 to mine column for chips with embeddings\n",
    "\n",
    "    if polygons2.empty:\n",
    "        for emb in negative_embeddings:\n",
    "            negative_embeddings['mine'] = 0 # Assign 0 to mine column for chips without overlap\n",
    "            data = pd.concat([data, negative_embeddings], ignore_index=True)\n",
    "    else:\n",
    "        for emb in negative_embeddings:\n",
    "            negative_chips = gpd.sjoin(negative_embeddings, polygons2, predicate='intersects',how = 'inner',rsuffix=\"_1\")\n",
    "            negative_chips = negative_chips[~negative_chips['index_right'].isna()]\n",
    "            negative_chips['mine'] = 0\n",
    "            data = pd.concat([data, negative_chips], ignore_index=True)\n",
    "\n",
    "    # If chips have both pit lakes and lakes, they will appear multiple times in data. \n",
    "    # Mark chip as positive if it has at least one pit lake, and drop duplicates.\n",
    "    data['mine'] = data.groupby('geometry')['mine'].transform('max')  \n",
    "\n",
    "    # Drop duplicates if you want to keep only one row per index\n",
    "    data = data.drop_duplicates(subset=['geometry'])\n",
    "    print(f'Number of positive samples: {(data['mine'] == 1).sum()}') \n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f485221",
   "metadata": {},
   "outputs": [],
   "source": [
    "data16R = LabelEmbeddings1_5(us16R_positive_embs_avg, us16R_negative_embs_avg, us_qa_pits_16, us_qa_nonlakes_16)\n",
    "data16S = LabelEmbeddings1_5(us16S_positive_embs_avg, us16S_negative_embs_avg, us_qa_pits_16, us_qa_nonlakes_16)\n",
    "data16T = LabelEmbeddings1_5(us16T_positive_embs_avg, us16T_negative_embs_avg, us_qa_pits_16, us_qa_nonlakes_16)\n",
    "\n",
    "print((data16R['mine'] == 1).sum())\n",
    "print((data16S['mine'] == 1).sum())\n",
    "print((data16T['mine'] == 1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4e3735",
   "metadata": {},
   "outputs": [],
   "source": [
    "data17R = LabelEmbeddings1_5(us17R_positive_embs_avg, us17R_negative_embs_avg, us_qa_pits_17, us_qa_nonlakes_17)\n",
    "data17S = LabelEmbeddings1_5(us17S_positive_embs_avg, us17S_negative_embs_avg, us_qa_pits_17, us_qa_nonlakes_17)\n",
    "data17T = LabelEmbeddings1_5(us17T_positive_embs_avg, us17T_negative_embs_avg, us_qa_pits_17, us_qa_nonlakes_17)\n",
    "\n",
    "print((data17R['mine'] == 1).sum())\n",
    "print((data17S['mine'] == 1).sum())\n",
    "print((data17T['mine'] == 1).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559feed4",
   "metadata": {},
   "source": [
    "### State by state approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ca16e7",
   "metadata": {},
   "source": [
    "#### Arizona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ba0a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get chips with mines and no mines in AZ from QA data\n",
    "az_data = LabelEmbeddings(mine_embeddings_az, non_mine_embeddings_az, az_qa_lakes)\n",
    "print(f\"Total samples: {len(az_data)}\") \n",
    "\n",
    "\n",
    "# Get chips with lakes in AZ\n",
    "az_lakes_chips = LabelEmbeddings(dummy, lake_embeddings_az, gpd.GeoDataFrame(), az_lakes)\n",
    "print(f\"Total samples: {len(az_lakes_chips)}\") \n",
    "\n",
    "az_data = pd.concat([az_data,az_lakes_chips], ignore_index=True) # Add lakes to AZ embeddings\n",
    "\n",
    "\n",
    "# Get chips for features identified as not lakes from QA\n",
    "az_nonlakes_chips = LabelEmbeddings(dummy, qa_non_mine_embeddings_az, gpd.GeoDataFrame(), az_qa_nonlakes)\n",
    "print(f\"Total samples: {len(az_nonlakes_chips)}\") \n",
    "\n",
    "az_data = pd.concat([az_data,az_nonlakes_chips], ignore_index=True) # Add not pit lakes to AZ embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09173a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get chips with pit lakes and non pit lakes in AZ \n",
    "az_data = LabelEmbeddings1_5(az_positive_embs_avg, az_negative_embs_avg, az_qa_pits, az_qa_nonlakes)\n",
    "\n",
    "print(\"Total samples:\") \n",
    "print(len(az_data))\n",
    "\n",
    "# Get chips with regular bodies of eater\n",
    "az_add_data = LabelEmbeddings1_5(az_positive_embs_avg, az_water_embs_avg, dnr_pit_lakes, az_lakes)\n",
    "\n",
    "non_mines = az_add_data[az_add_data['mine'] == 0]\n",
    "\n",
    "az_data = pd.concat([az_data, non_mines])\n",
    "az_data = az_data.drop_duplicates('geometry')\n",
    "\n",
    "print(\"Total samples:\") \n",
    "print(len(az_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c799490e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get chips with mines and no mines in AZ\n",
    "az_data = LabelEmbeddings(mine_embeddings_az, non_mine_embeddings_az, az_mines_intersect)\n",
    "\n",
    "print(\"Total samples:\") \n",
    "print(len(az_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd100de",
   "metadata": {},
   "source": [
    "#### Indiana and KY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a699e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get chips with pit lakes and non pit lakes in IN\n",
    "\n",
    "in_data = LabelEmbeddings1_5(in_positive_embs_avg, in_negative_embs_avg, in_qa_pits, in_qa_nonlakes)\n",
    "\n",
    "print(\"Total samples:\") \n",
    "print(len(in_data))\n",
    "\n",
    "# Get chips with regular bodies of eater\n",
    "#in_add_data = LabelEmbeddings1_5(az_positive_embs_avg, az_water_embs_avg, dnr_pit_lakes, az_lakes)\n",
    "\n",
    "#non_mines = az_add_data[az_add_data['mine'] == 0]\n",
    "#in_data = pd.concat([in_data, non_mines])\n",
    "\n",
    "in_data = in_data.drop_duplicates('geometry')\n",
    "\n",
    "print(\"Total samples:\") \n",
    "print(len(in_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66139de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get chips with pit lakes and non pit lakes in KY \n",
    "\n",
    "ky_data = LabelEmbeddings1_5(ky_positive_embs_avg, ky_negative_embs_avg, ky_qa_pits, ky_qa_nonlakes)\n",
    "\n",
    "print(\"Total samples:\") \n",
    "print(len(ky_data))\n",
    "\n",
    "# Get chips with regular bodies of eater\n",
    "#in_add_data = LabelEmbeddings1_5(az_positive_embs_avg, az_water_embs_avg, dnr_pit_lakes, az_lakes)\n",
    "\n",
    "#non_mines = az_add_data[az_add_data['mine'] == 0]\n",
    "#in_data = pd.concat([in_data, non_mines])\n",
    "\n",
    "ky_data = ky_data.drop_duplicates('geometry')\n",
    "\n",
    "print(\"Total samples:\") \n",
    "print(len(ky_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c01ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get chips with pit lakes and regular bodies of water in MN in DNR dataset\n",
    "mn_data = LabelEmbeddings1_5(mn_positive_embs_avg, mn_water_embs_avg, dnr_pit_lakes, dnr_water_cropped)\n",
    "\n",
    "print(\"Total samples:\") \n",
    "print(len(mn_data))\n",
    "\n",
    "# Add QA'ed non lakes\n",
    "mn_add_data = LabelEmbeddings1_5(mn_positive_embs_avg, mn_negative_embs_avg, dnr_pit_lakes, mn_qa_nonlakes)\n",
    "\n",
    "non_mines = mn_add_data[mn_add_data['mine'] == 0]\n",
    "\n",
    "mn_data = pd.concat([mn_data, non_mines])\n",
    "print(\"Total samples:\") \n",
    "print(len(mn_data))\n",
    "\n",
    "# Add in pit lakes that are only in the Aquarry QA dataset. Not removing the extra land area for now\n",
    "intersecting_aquarry_dnr = gpd.sjoin(mn_qa_pits, dnr_pit_lakes, how='left', predicate='intersects').drop_duplicates('geometry')\n",
    "aquarry_only_pits = mn_qa_pits[intersecting_aquarry_dnr['index_right'].isna()]\n",
    "print(f'Pits only in QA dataset: {len(aquarry_only_pits)}')\n",
    "\n",
    "mn_add_data = LabelEmbeddings1_5(mn_positive_embs_avg, mn_negative_embs_avg, aquarry_only_pits, mn_qa_nonlakes)\n",
    "\n",
    "aquarry_mn_mines = mn_add_data[mn_add_data['mine'] == 1]\n",
    "\n",
    "mn_data = pd.concat([mn_data, aquarry_mn_mines])\n",
    "mn_data = mn_data.drop_duplicates('geometry')\n",
    "\n",
    "print(\"Total samples:\") \n",
    "print(len(mn_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ead17dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mn_data.to_crs(epsg=32615).iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216eb25e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7cbbc689",
   "metadata": {},
   "source": [
    "### Rasterize polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1abe45b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function that assigns corresponding polygons to each chip\n",
    "from shapely.ops import unary_union\n",
    "\n",
    "def GetPolygonsforChips(embeddings, pit_lakes):\n",
    "    \"\"\"Get intersecting pit lakes for each embedding chip geometry\n",
    "    \"\"\"\n",
    "    embeddings['pit_lake_poly'] = None\n",
    "    embeddings = embeddings.reset_index(drop=True)\n",
    "\n",
    "    for idx, chip in embeddings.iterrows():\n",
    "            intersecting = pit_lakes[pit_lakes.intersects(chip['geometry'])]\n",
    "            embeddings.at[idx, 'pit_lake_poly'] = intersecting['geometry'].tolist()     \n",
    "\n",
    "    embeddings = embeddings[embeddings['pit_lake_poly'].apply(lambda x: len(x) > 0)] # drop empty rows\n",
    "    embeddings['pit_lake_poly'] = embeddings['pit_lake_poly'].apply(lambda x: unary_union(x)) # Get multipolygons from a list\n",
    "    embeddings['pit_lake_poly'] = gpd.GeoSeries(embeddings['pit_lake_poly'], crs = 4326).to_crs(epsg=32615) # Convert geometries to epsg:32615\n",
    "    embeddings = embeddings.to_crs(epsg=32615) # Convert geometries to epsg:32615\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "351d82a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to take chip embeddings with corresponding polygons and rasterize\n",
    "from rasterio.features import rasterize\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "def RasterizePolygons(data_with_polys):\n",
    "    \"\"\" Get rasterized polygons\n",
    "\n",
    "    Parameters:\n",
    "    data_with_polys (GeoDataFrame): Geodataframe with embedding geometries as geometry \n",
    "    and with multipolygons of pit lakes that intersect with each chip\n",
    "\n",
    "    Outputs:\n",
    "    rasters (list): list of numpy raster arrays for each chip\n",
    "    \"\"\"\n",
    "    \n",
    "    rasters = []\n",
    "\n",
    "    for idx, chip in data_with_polys.iterrows():\n",
    "\n",
    "        # Define raster dimensions based on chip's bounding box\n",
    "        bounds = chip.geometry.bounds\n",
    "\n",
    "        resolution = 10\n",
    "        width = int((bounds[2] - bounds[0]) / resolution)\n",
    "        height = int((bounds[3] - bounds[1]) / resolution)\n",
    "        \n",
    "        # Define raster transform\n",
    "        transform = rasterio.transform.from_bounds(*bounds, width, height)\n",
    "        \n",
    "        # Create a list of geometries for rasterization\n",
    "        geometries = [chip['pit_lake_poly']]\n",
    "        print(geometries)\n",
    "        print(height, width)\n",
    "        \n",
    "        # Rasterize the pit lakes\n",
    "        chip_raster = rasterize(\n",
    "            geometries,\n",
    "            out_shape=(height, width),\n",
    "            transform=transform,\n",
    "            fill=0,\n",
    "            dtype='uint8'\n",
    "        )\n",
    "\n",
    "        rasters.append(chip_raster)\n",
    "\n",
    "    return rasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "75b1edf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Createxarray(rasters, embs_with_polys):\n",
    "    \"\"\"Convert list of chip rasters to xarrays\n",
    "    \"\"\"\n",
    "    rasters_xr = []\n",
    "\n",
    "    for i in range(len(rasters)):\n",
    "        lon_min, lat_min, lon_max, lat_max = embs_with_polys.geometry[i].bounds\n",
    "\n",
    "        # Generate latitude and longitude arrays\n",
    "        latitudes = np.linspace(lat_max, lat_min, rasters[i].shape[0])\n",
    "        longitudes = np.linspace(lon_min, lon_max, rasters[i].shape[1])\n",
    "\n",
    "        # Create xarray DataArray\n",
    "        data_array = xr.DataArray(\n",
    "            rasters[i],\n",
    "            dims=[\"lat\", \"lon\"],\n",
    "            coords={\"lat\": latitudes, \"lon\": longitudes},\n",
    "            name=\"pit_lakes\" \n",
    "        )\n",
    "\n",
    "        # Create xarray Dataset\n",
    "        dataset = xr.Dataset({\"pit_lakes\": data_array})\n",
    "        rasters_xr.append(dataset)\n",
    "\n",
    "    return rasters_xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "id": "867eb9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minnesota\n",
    "chips_with_polygons = mn_data[mn_data['mine']==1] # Ensure this is right mn_data\n",
    "chips_with_polygons = chips_with_polygons.drop_duplicates('geometry')\n",
    "mn_data_with_polys = GetPolygonsforChips(chips_with_polygons, dnr_pit_lakes)\n",
    "\n",
    "embs_with_polys = pd.concat([mn_data_with_polys,az_data_with_polys]).reset_index(drop=True)\n",
    "\n",
    "# Aquarry mine testing\n",
    "chips_with_polygons = aquarry_mn_mines.drop_duplicates('geometry')\n",
    "aq_data_with_polys = GetPolygonsforChips(chips_with_polygons, aquarry_only_pits)\n",
    "embs_with_polys = aq_data_with_polys\n",
    "rasters = RasterizePolygons(embs_with_polys)\n",
    "rasters_xr = Createxarray(rasters, embs_with_polys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393c4748",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "# Get intersecting pit lakes and rasterize\n",
    "us_data_mines16 = pd.concat([data16R, data16S, data16T]).drop_duplicates('geometry')\n",
    "\n",
    "us_chips_with_polygons16 = us_data_mines16\n",
    "us_embs_with_polys16 = GetPolygonsforChips(us_chips_with_polygons16, us_qa_pits_16) # drops rows without intersection\n",
    "us_rasters16 = RasterizePolygons(us_embs_with_polys16)\n",
    "us_embs_with_polys16 = us_embs_with_polys16.reset_index(drop=True)\n",
    "us_rasters_xr16 = Createxarray(us_rasters16, us_embs_with_polys16) # Uncomment when running normally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db39e56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get intersecting pit lakes and rasterize\n",
    "\n",
    "us_data_mines17 = pd.concat([data17R, data17S, data17T])\n",
    "\n",
    "us_chips_with_polygons17 = us_data_mines17.drop_duplicates('geometry')\n",
    "us_embs_with_polys17 = GetPolygonsforChips(us_chips_with_polygons17, us_qa_pits_17) # drops empty rows\n",
    "us_rasters17 = RasterizePolygons(us_embs_with_polys17)\n",
    "us_embs_with_polys17 = us_embs_with_polys17.reset_index(drop=True)\n",
    "us_rasters_xr17 = Createxarray(us_rasters17, us_embs_with_polys17) # Uncomment when running normally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4b665a",
   "metadata": {},
   "source": [
    "### NDWI Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "972c025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize GDAL settings for cloud optimized reading\n",
    "def SearchSTAC(aoi, start, end):\n",
    "    os.environ[\"GDAL_DISABLE_READDIR_ON_OPEN\"] = \"EMPTY_DIR\"\n",
    "    os.environ[\"AWS_REQUEST_PAYER\"] = \"requester\"\n",
    "\n",
    "    STAC_API = \"https://earth-search.aws.element84.com/v1\"\n",
    "    COLLECTION = \"sentinel-2-l2a\"\n",
    "\n",
    "    # Search the catalogue\n",
    "    catalog = pystac_client.Client.open(STAC_API)\n",
    "    search = catalog.search(\n",
    "        collections=[COLLECTION],\n",
    "        datetime=f\"{start}/{end}\",\n",
    "        intersects=aoi,\n",
    "        #max_items=,\n",
    "        query={\"eo:cloud_cover\": {\"lt\": 10}},\n",
    "    )\n",
    "\n",
    "    all_items = search.get_all_items()\n",
    "    print(len(all_items))\n",
    "\n",
    "    # Reduce to one per date (there might be some duplicates\n",
    "    # based on the location)\n",
    "    items = []\n",
    "    dates = []\n",
    "    for item in all_items:\n",
    "        if item.datetime.date() not in dates:\n",
    "            items.append(item)\n",
    "            dates.append(item.datetime.date())\n",
    "\n",
    "    print(f\"Found {len(items)} items\")\n",
    "    return items, dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d77def50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the pixel values for the bounding box in the target projection.\n",
    "def RetrievePixels(items,aoi_bounds):\n",
    "    dask.config.set({\"array.slicing.split_large_chunks\": True})\n",
    "    stack = None\n",
    "\n",
    "    if len(items) != 0:\n",
    "        epsg = items[0].properties[\"proj:epsg\"]\n",
    "        gsd = 10\n",
    "        valid_stacks = []\n",
    "\n",
    "        bounds_gdf = gpd.GeoDataFrame(geometry=[aoi_bounds], crs='EPSG:32615')\n",
    "        #bounds_gdf = bounds_gdf.to_crs(epsg=epsg)\n",
    "        transformed_bbox = bounds_gdf.geometry[0].bounds\n",
    "\n",
    "    for item in items:\n",
    "        try:\n",
    "            # Attempt to create the stack for the current item\n",
    "            stack = stackstac.stack(\n",
    "                [item],  # Process one item at a time\n",
    "                snap_bounds=True,\n",
    "                epsg=32615,\n",
    "                resolution=gsd,\n",
    "                dtype=\"float32\",\n",
    "                rescale=False,\n",
    "                fill_value=np.float32(0),\n",
    "                assets=[\"green\", \"nir\"],\n",
    "                resampling=Resampling.nearest,\n",
    "                chunksize=512,\n",
    "                bounds=transformed_bbox\n",
    "            )\n",
    "            valid_stacks.append(stack)\n",
    "        except RuntimeError as e:\n",
    "            # Log and skip the problematic item\n",
    "            print(f'Error creating stack for item {item.id}: {e}')\n",
    "            continue\n",
    "\n",
    "        if len(valid_stacks) != 0:\n",
    "            combined_stack = dask.array.concatenate(valid_stacks, axis=0).compute()\n",
    "            stack = stack.compute()\n",
    "        \n",
    "    return stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "73abfd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate NDWI and create water mask \n",
    "def CalculateNDWI(image):\n",
    "    ndwi = image\n",
    "    green = image.sel(band=\"green\")\n",
    "    nir = image.sel(band=\"nir\")\n",
    "\n",
    "    ndwi = (green - nir) / (green + nir)\n",
    "    return ndwi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "763dbed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FilterRasters(rasters, embs_with_polys):\n",
    "    # Date range\n",
    "    start = \"2024-07-01\"    \n",
    "    end = \"2024-09-15\"\n",
    "\n",
    "    for i in range(len(rasters)):\n",
    "        \n",
    "        #Get NDWI\n",
    "        aoi_epsg4326 = embs_with_polys['geometry'].to_crs(epsg=4326)[i] \n",
    "        bounds = embs_with_polys['geometry'][i]    # Should be in epsg:32615\n",
    "\n",
    "        ndwiimages, ndwidates = SearchSTAC(aoi_epsg4326, start, end) \n",
    "\n",
    "        try: \n",
    "            ndwistack = RetrievePixels(ndwiimages, bounds)\n",
    "            summer_composite = ndwistack.groupby('band').median(dim = 'time')\n",
    "            ndwi_summer = CalculateNDWI(summer_composite)\n",
    "\n",
    "            # Compute mask\n",
    "            water_mask = xr.where(ndwi_summer > 0.1, 1, 0)\n",
    "\n",
    "            # Account for small differences\n",
    "            # Create new evenly spaced coordinates and interploate\n",
    "            if water_mask.x.shape[0] != water_mask.y.shape[0]:\n",
    "                new_coords = np.linspace(water_mask.coords['x'][0], water_mask.coords['x'][-2], water_mask.x.shape[0])\n",
    "\n",
    "                water_mask_resampled = water_mask.interp(x=new_coords)\n",
    "            else:\n",
    "                water_mask_resampled = water_mask\n",
    "\n",
    "            # Rename coords to match\n",
    "            water_mask_resampled = water_mask_resampled.rename({'y': 'lat', 'x': 'lon'})\n",
    "            print(water_mask_resampled.sum().values)\n",
    "            # Resample to make water mask match \n",
    "\n",
    "            water_mask_resampled = water_mask_resampled.interp(lat=rasters[i]['pit_lakes']['lat'].values,lon=rasters[i]['pit_lakes']['lon'].values, method='nearest')\n",
    "\n",
    "            print(water_mask_resampled.sum().values)\n",
    "    \n",
    "            mask = (rasters[i]['pit_lakes'] == 1).compute()\n",
    "            print(mask.sum().values)\n",
    "\n",
    "            rasters[i]['filtered_pit_lakes'] = rasters[i]['pit_lakes']\n",
    "            rasters[i]['filtered_pit_lakes'] = xr.where(mask, water_mask_resampled, rasters[i]['pit_lakes'])\n",
    "            print(rasters[i]['filtered_pit_lakes'].shape)\n",
    "            print(rasters[i]['filtered_pit_lakes'].sum().values)\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            print(f'Error filtering raster {i}')\n",
    "            continue\n",
    "        \n",
    "        except AttributeError:\n",
    "            print(f'No images found for {i}')\n",
    "            continue\n",
    "\n",
    "    return rasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3c538a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get filtered polygons for US 16\n",
    "us_results16 = FilterRasters(us_rasters_xr16, us_embs_with_polys16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4302a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get filtered polygons for US 17\n",
    "us_results17 = FilterRasters(us_rasters_xr17, us_embs_with_polys17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df038914",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attempt to store, unfinished\n",
    "\n",
    "combined1 = xr.concat(us_results[0:150], dim=\"band\")  # Combine along a new dimension \"band\"\n",
    "combined1 = combined1.assign_coords(band=range(1, 150 + 1))  # Assign band numbers\n",
    "\n",
    "# Write to a multi-band GeoTIFF\n",
    "combined1.rio.write_crs(\"EPSG:32615\", inplace=True)  # Set CRS\n",
    "combined1.rio.to_raster(\"data/us_rasters_xr16_1.tif\")\n",
    "\n",
    "combined2 = xr.concat(us_results[150:300], dim=\"band\")  # Combine along a new dimension \"band\"\n",
    "combined2 = combined2.assign_coords(band=range(151, 300 + 1))  # Assign band numbers\n",
    "\n",
    "# Write to a multi-band GeoTIFF\n",
    "combined2.rio.write_crs(\"EPSG:32615\", inplace=True)  # Set CRS\n",
    "combined2.rio.to_raster(\"data/us_rasters_xr16_2.tif\")\n",
    "\n",
    "\n",
    "combined1 = xr.concat(us_results17[0:130], dim=\"band\")  # Combine along a new dimension \"band\"\n",
    "combined1 = combined1.assign_coords(band=range(1, 130 + 1))  # Assign band numbers\n",
    "combined1.rio.write_crs(\"EPSG:32615\", inplace=True)  # Set CRS\n",
    "combined1.rio.to_raster(\"data/us_rasters_xr17_1.tif\")\n",
    "\n",
    "combined2 = xr.concat(us_results17[130:267], dim=\"band\")  # Combine along a new dimension \"band\"\n",
    "combined2 = combined2.assign_coords(band=range(131, 267 + 1))  # Assign band numbers\n",
    "combined2.rio.write_crs(\"EPSG:32615\", inplace=True)  # Set CRS\n",
    "combined2.rio.to_raster(\"data/us_rasters_xr17_2.tif\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 982,
   "id": "ee8f848d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visually inspect NDWI filters\n",
    "for i in range(0,4):\n",
    "    #us_results[i] = us_results[i].rename({'lon': 'x', 'lat': 'y'})\n",
    "    us_results16[i]['filtered_pit_lakes'].rio.write_crs(\"EPSG:32615\", inplace=True)  # Set CRS\n",
    "    us_results16[i]['filtered_pit_lakes'].rio.to_raster(f\"data/filtered_lakes_{i + 1}.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "8e0da504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample to 256 x 256\n",
    "def TrainingResample(rasters):\n",
    "    rasters_list = []\n",
    "\n",
    "    for i in range(len(rasters)):\n",
    "        lat_min = float(rasters[i]['filtered_pit_lakes']['lat'].min().values)\n",
    "        lat_max = float(rasters[i]['filtered_pit_lakes']['lat'].max().values)\n",
    "        lon_min = float(rasters[i]['filtered_pit_lakes']['lon'].min().values)\n",
    "        lon_max = float(rasters[i]['filtered_pit_lakes']['lon'].max().values)\n",
    "        new_lat = np.linspace(lat_min, lat_max, 256)\n",
    "        new_lon = np.linspace(lon_min, lon_max, 256)\n",
    "\n",
    "        # Interpolate to the new grid\n",
    "        resampled = rasters[i]['filtered_pit_lakes'].interp(lat=new_lat, lon=new_lon, method='nearest')\n",
    "        rasters_list.append(resampled)\n",
    "\n",
    "    return rasters_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7322d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE?\n",
    "# Get only those with results and resize\n",
    "filtered_results16 = [xr for xr in us_results16 if 'filtered_pit_lakes' in xr.data_vars] # If images were found\n",
    "filtered_results16 = [arr for arr in filtered_results16 if arr['filtered_pit_lakes'].values.any()] # If array is non 0\n",
    "\n",
    "resampled_us = TrainingResample(filtered_results)\n",
    "resampled_us[0].plot()\n",
    "\n",
    "# Get only those with results and resize\n",
    "filtered_results17 = [xr for xr in us_results17 if 'filtered_pit_lakes' in xr.data_vars] # If images were found\n",
    "filtered_results17 = [arr for arr in filtered_results17 if arr['filtered_pit_lakes'].values.any()] # If array is non 0\n",
    "\n",
    "resampled_us17 = TrainingResample(filtered_results)\n",
    "resampled_us17[0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b56b449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any rasters without any pit lake intersections\n",
    "final_indices = []\n",
    "us_embs_with_polys_reset16 = us_embs_with_polys16.reset_index(drop=True) \n",
    "\n",
    "# Check for rasters without pit lakes\n",
    "for i, xr in enumerate(us_results16):\n",
    "    # Step 1: Check if 'filtered_pit_lakes' exists in data_vars\n",
    "    if 'filtered_pit_lakes' in xr.data_vars:\n",
    "        # Step 2: Check if 'filtered_pit_lakes' has non-zero values\n",
    "        if xr['filtered_pit_lakes'].values.any():\n",
    "            final_indices.append(i)\n",
    "\n",
    "us_results16_clean = [xr for i, xr in enumerate(us_results16) if i in final_indices]\n",
    "\n",
    "resampled_us16 = TrainingResample(us_results16_clean)\n",
    "\n",
    "# Drop rows in us_embs_with_polys not corresponding to final indices\n",
    "us_embs_with_polys_dropped16 = us_embs_with_polys_reset16.loc[final_indices]\n",
    "print(len(us_embs_with_polys_dropped16))\n",
    "print(len(resampled_us16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4bfbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any rasters without any pit lake intersections\n",
    "final_indices = []\n",
    "us_embs_with_polys_reset17 = us_embs_with_polys17.reset_index(drop=True) \n",
    "\n",
    "# Check for rasters without pit lakes\n",
    "for i, xr in enumerate(us_results17):\n",
    "    # Step 1: Check if 'filtered_pit_lakes' exists in data_vars\n",
    "    if 'filtered_pit_lakes' in xr.data_vars:\n",
    "        # Step 2: Check if 'filtered_pit_lakes' has non-zero values\n",
    "        if xr['filtered_pit_lakes'].values.any():\n",
    "            final_indices.append(i)\n",
    "\n",
    "us_results17_clean = [xr for i, xr in enumerate(us_results17) if i in final_indices]\n",
    "\n",
    "resampled_us17 = TrainingResample(us_results17_clean)\n",
    "\n",
    "# Drop rows in us_embs_with_polys not corresponding to final indices\n",
    "us_embs_with_polys_dropped17 = us_embs_with_polys_reset17.loc[final_indices]\n",
    "print(len(us_embs_with_polys_dropped17))\n",
    "print(len(resampled_us17))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a7a259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into seasonal\n",
    "q1stack, q1dates = SearchSTAC(aoi,\"2024-01-01\", \"2024-03-31\")\n",
    "q2stack, q2dates = SearchSTAC(aoi,\"2024-04-01\", \"2024-06-30\")\n",
    "q3stack, q3dates = SearchSTAC(aoi,\"2024-07-01\", \"2024-09-30\")\n",
    "q4stack, q4dates = SearchSTAC(aoi,\"2024-10-01\", \"2024-12-31\")\n",
    "\n",
    "stack1 = RetrievePixels(q1stack)\n",
    "stack2 = RetrievePixels(q2stack)\n",
    "stack3 = RetrievePixels(q3stack)\n",
    "stack4 = RetrievePixels(q4stack)\n",
    "\n",
    "# Get median of values across time for different bands for each season\n",
    "import xarray as xr\n",
    "\n",
    "q1_composite = stack1.groupby('band').median(dim = 'time')\n",
    "q2_composite = stack2.groupby('band').median(dim = 'time')\n",
    "q3_composite = stack3.groupby('band').median(dim = 'time')\n",
    "q4_composite = stack4.groupby('band').median(dim = 'time')\n",
    "\n",
    "\n",
    "ndwi_q1 = CalculateNDWI(q1_composite)\n",
    "ndwi_q2 = CalculateNDWI(q2_composite)\n",
    "ndwi_q3 = CalculateNDWI(q3_composite)\n",
    "ndwi_q4 = CalculateNDWI(q4_composite)\n",
    "\n",
    "ndwi_ds = [ndwi_q1, ndwi_q2, ndwi_q3, ndwi_q4]\n",
    "\n",
    "for i in range(len(ndwi_ds)):\n",
    "    if 's2:processing_baseline' in ndwi_ds[i].coords:\n",
    "        ndwi_ds[i] = ndwi_ds[i].reset_coords('s2:processing_baseline', drop=True)\n",
    "        \n",
    "annual_median = xr.concat(ndwi_ds, dim=\"time\").median(dim='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da55e9c",
   "metadata": {},
   "source": [
    "### Train a CNN to predict pixel masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "46353b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EmbeddingToRasterCNN(nn.Module):\n",
    "    def __init__(self, embedding_dim=1024, output_size=(256, 256)):\n",
    "        super(EmbeddingToRasterCNN, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # Fully connected layer to project embedding to a smaller feature map\n",
    "        self.fc = nn.Linear(embedding_dim, 8 * 8 * 128)  # 8x8 feature map with 128 channels\n",
    "\n",
    "        # Transpose convolution layers to upsample the feature map progressively\n",
    "        self.deconv1 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)  # 8x8 -> 16x16\n",
    "        self.deconv2 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)   # 16x16 -> 32x32\n",
    "        self.deconv3 = nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1)   # 32x32 -> 64x64\n",
    "        self.deconv4 = nn.ConvTranspose2d(16, 8, kernel_size=4, stride=2, padding=1)    # 64x64 -> 128x128\n",
    "        self.deconv5 = nn.ConvTranspose2d(8, 1, kernel_size=4, stride=2, padding=1)     # 128x128 -> 256x256\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Fully connected layer to create an initial feature map\n",
    "        x = self.fc(x)\n",
    "        x = F.relu(x)\n",
    "        x = x.view(-1, 128, 8, 8)  # Reshape to (batch_size, channels, height, width)\n",
    "\n",
    "        # Upsample using transposed convolutions\n",
    "        x = F.relu(self.deconv1(x))  # 16x16\n",
    "        x = F.relu(self.deconv2(x))  # 32x32\n",
    "        x = F.relu(self.deconv3(x))  # 64x64\n",
    "        x = F.relu(self.deconv4(x))  # 128x128\n",
    "        x = self.deconv5(x) \n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "05d163b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class EmbeddingRasterDataset(Dataset):\n",
    "    \"\"\"\n",
    "        Initializes the dataset with inputs (embeddings) and targets (rasters).\n",
    "\n",
    "        Args:\n",
    "            embeddings (numpy.ndarray or torch.Tensor): Input embeddings, shape (num_samples, 1024).\n",
    "            rasters (numpy.ndarray or torch.Tensor): Target rasters, shape (num_samples, 256, 256).\n",
    "    \"\"\"\n",
    "    def __init__(self, embeddings, rasters):\n",
    "        self.embeddings = embeddings\n",
    "        self.rasters = rasters\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        embedding = self.embeddings[idx]\n",
    "        raster = self.rasters[idx]\n",
    "        return torch.tensor(embedding, dtype=torch.float32), torch.tensor(raster, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dbb73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE?\n",
    "index_to_drop = us_embs_with_polys_dropped_aug[us_embs_with_polys_dropped_aug['embeddings_array'].isna() == True].index[0]\n",
    "us_embs_with_polys_dropped_aug = us_embs_with_polys_dropped_aug.drop(index = index_to_drop)\n",
    "\n",
    "resampled_us_dropped = resampled_us[:index_to_drop] + resampled_us[index_to_drop+1:]\n",
    "\n",
    "print(len(resampled_us_dropped))\n",
    "print(len(us_embs_with_polys_dropped_aug))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c42e0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16\n",
    "#resampled_us_256 = TrainingResample(filtered_results)\n",
    "\n",
    "index_to_drop = us_embs_with_polys_dropped_aug[us_embs_with_polys_dropped_aug['embeddings_array'].isna() == True].index[0]\n",
    "us_embs_with_polys_dropped_aug = us_embs_with_polys_dropped_aug.drop(index = index_to_drop)\n",
    "\n",
    "resampled_us_dropped_256 = resampled_us_256[:index_to_drop] + resampled_us_256[index_to_drop+1:]\n",
    "\n",
    "print(len(resampled_us_dropped_256))\n",
    "print(len(us_embs_with_polys_dropped_aug))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db102c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16 Clean training data and convert list of arrays to array\n",
    "resampled_us16_np = resampled_us16\n",
    "for i in range(len(resampled_us16)):\n",
    "    resampled_us16_np[i] = np.array(resampled_us16[i])\n",
    "\n",
    "resampled_us16_np = np.where(np.isnan(resampled_us16_np), 0, resampled_us16_np) \n",
    "resampled_us16_np = np.round_(resampled_us16_np)\n",
    "resampled_us16_np = np.int_(resampled_us16_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "id": "c6aed22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17 Clean training data and convert list of arrays to array\n",
    "resampled_us17_np = resampled_us17\n",
    "for i in range(len(resampled_us17)):\n",
    "    resampled_us17_np[i] = np.array(resampled_us17[i])\n",
    "\n",
    "resampled_us17_np = np.where(np.isnan(resampled_us17_np), 0, resampled_us17_np) \n",
    "resampled_us17_np = np.round_(resampled_us17_np)\n",
    "resampled_us17_np = np.int_(resampled_us17_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "id": "860c0b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#16 Prep training data\n",
    "\n",
    "us_embs_with_polys_dropped16 = us_embs_with_polys_dropped16.reset_index(drop=True)\n",
    "\n",
    "index = sorted(random.sample(range(0, len(resampled_us16_np)), 25))\n",
    "\n",
    "train_embs_with_polys16 = us_embs_with_polys_dropped16.drop(index = index)\n",
    "train_embs_array16 = np.vstack(train_embs_with_polys16['embeddings'])\n",
    "train_raster_array16 = np.stack([item for idx, item in enumerate(resampled_us16_np) if idx not in index])\n",
    "\n",
    "test_embs_with_polys16 = us_embs_with_polys_dropped16.iloc[index]\n",
    "test_embs_array16 = np.vstack(us_embs_with_polys_dropped16['embeddings'][index])\n",
    "test_raster_array16 = np.stack([item for idx, item in enumerate(resampled_us16_np) if idx in index])\n",
    "\n",
    "#train_raster_array_16 = np.where(np.isnan(train_raster_array), 0, train_raster_array) \n",
    "#train_raster_array_16 = np.round_(train_raster_array_filled16)\n",
    "#train_raster_array_16 = np.int_(train_raster_array_filled_r16)\n",
    "#test_raster_array16 = np.round_(test_raster_array16)\n",
    "#test_raster_array16 = np.int_(test_raster_array16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "id": "d33b867d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#17 Prep training data\n",
    "us_embs_with_polys_dropped17 = us_embs_with_polys_dropped17.reset_index(drop=True)\n",
    "\n",
    "index = sorted(random.sample(range(0, len(us_embs_with_polys_dropped17)), 25))\n",
    "\n",
    "train_embs_with_polys17 = us_embs_with_polys_dropped17.drop(index = index)\n",
    "train_embs_array17 = np.vstack(train_embs_with_polys17['embeddings'])\n",
    "train_raster_array17 = np.stack([item for idx, item in enumerate(resampled_us17_np) if idx not in index])\n",
    "\n",
    "test_embs_with_polys17 = us_embs_with_polys_dropped17.iloc[index]\n",
    "test_embs_array17 = np.vstack(us_embs_with_polys_dropped17['embeddings'][index])\n",
    "test_raster_array17 = np.stack([item for idx, item in enumerate(resampled_us17_np) if idx in index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04f2ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control for error\n",
    "#test_raster_array16 = np.where(test_raster_array16 == -2147483648, 0, test_raster_array16) \n",
    "\n",
    "print(np.unique(train_raster_array16, return_counts = True))\n",
    "print(np.unique(test_raster_array16, return_counts = True))\n",
    "print(np.unique(train_raster_array17, return_counts = True))\n",
    "print(np.unique(test_raster_array17, return_counts = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e024c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embs_array1617 = np.concatenate((train_embs_array16, train_embs_array17), axis = 0)\n",
    "train_raster_array1617 = np.concatenate((train_raster_array16, train_raster_array17), axis = 0)\n",
    "\n",
    "test_raster_array1617 = np.concatenate((test_raster_array16, test_raster_array17), axis = 0)\n",
    "test_embs_array1617 = np.concatenate((test_embs_array16, test_embs_array17), axis = 0)\n",
    "\n",
    "train_embs_with_polys1617 = pd.concat([train_embs_with_polys16, train_embs_with_polys17])\n",
    "test_embs_with_polys1617 = pd.concat([test_embs_with_polys16, test_embs_with_polys17])\n",
    "\n",
    "print(train_embs_array1617.shape)\n",
    "print(test_raster_array1617.shape)\n",
    "print(len(train_embs_with_polys1617))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929b0577",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_train_data = EmbeddingRasterDataset(train_embs_array1617, train_raster_array1617)\n",
    "\n",
    "print(cnn_train_data.embeddings.shape)\n",
    "print(cnn_train_data.rasters.shape)\n",
    "print(test_raster_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e7b956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop out samples for 16 and 17 for ablation study\n",
    "\n",
    "# Indexing from training embeddings, holding test embeddings constant\n",
    "index = sorted(random.sample(range(0, len(train_embs_array1617)), 275))\n",
    "train_embs_array1617_dropped = train_embs_with_polys1617.reset_index(drop = True)\n",
    "\n",
    "train_embs_with_polys1617_dropped = train_embs_array1617_dropped.drop(index = index)\n",
    "train_embs_array1617_dropped = train_embs_array1617[index]\n",
    "train_raster_array1617_dropped = train_raster_array1617[index]\n",
    "\n",
    "cnn_train_data = EmbeddingRasterDataset(train_embs_array1617_dropped, train_raster_array1617_dropped)\n",
    "\n",
    "print(cnn_train_data.embeddings.shape)\n",
    "print(cnn_train_data.rasters.shape)\n",
    "print(test_raster_array1617.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f55dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rasters with less than 65 positive pixels\n",
    "\n",
    "valid_indices = []\n",
    "\n",
    "# Iterate through each 256x256 slice in 'array'\n",
    "for i in range(train_raster_array1617.shape[0]):\n",
    "    if np.sum(train_raster_array1617[i]) >= 65:\n",
    "        valid_indices.append(i)\n",
    "\n",
    "# Convert valid_indices to numpy array (optional)\n",
    "valid_indices = np.array(valid_indices)\n",
    "\n",
    "# Filter 'array' and 'train_embs_array' using valid_indices\n",
    "filtered_raster_array = train_raster_array1617[valid_indices]\n",
    "filtered_train_embs_array = train_embs_array1617[valid_indices]\n",
    "filtered_train_embs_with_polys = train_embs_with_polys1617.iloc[valid_indices]\n",
    "\n",
    "print(\"Shape of filtered array:\", filtered_raster_array.shape)\n",
    "print(\"Shape of filtered train_embs_array:\", filtered_train_embs_array.shape)\n",
    "\n",
    "cnn_train_data = EmbeddingRasterDataset(filtered_train_embs_array, filtered_raster_array)\n",
    "\n",
    "\n",
    "print(cnn_train_data.embeddings.shape)\n",
    "print(cnn_train_data.rasters.shape)\n",
    "print(test_raster_array.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12703ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Assuming embeddings and rasters are numpy arrays of shape (num_samples, 1024) and (num_samples, 256, 256)\n",
    "dataloader = DataLoader(cnn_train_data, batch_size=10, shuffle=True)\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = EmbeddingToRasterCNN().to(device)\n",
    "num_positives = cnn_train_data.rasters.sum()\n",
    "num_negatives = cnn_train_data.rasters.size - num_positives  # Total pixels - positives\n",
    "#class_weight = min(num_negatives / num_positives, 100)  # Cap the weight\n",
    "class_weight = (num_negatives / num_positives) / 2\n",
    "print(class_weight)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([class_weight]).to(device))\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')  # He initialization\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)  # Initialize biases to zero\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "\n",
    "# Debugging\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "\n",
    "def check_for_nans(module, input, output):\n",
    "    print(f\"Checking {module}\")\n",
    "    print(f\"Output has NaNs? {torch.isnan(output).any()}\")\n",
    "\n",
    "hooks = []\n",
    "for layer in model.modules():\n",
    "    hook = layer.register_forward_hook(check_for_nans)\n",
    "    hooks.append(hook)  # Keep track of the hooks\n",
    "# Remove all hooks\n",
    "for hook in hooks:\n",
    "    hook.remove()\n",
    "\n",
    "losses = []\n",
    "# Training loop\n",
    "epochs = 30\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (embeddings_batch, rasters_batch) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #print(torch.isnan(rasters_batch).any())  # Check if rasters have NaN\n",
    "        #print(torch.isnan(embeddings_batch).any())\n",
    "\n",
    "        # Move data to device\n",
    "        embeddings_batch = embeddings_batch.to(device)\n",
    "        rasters_batch = rasters_batch.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(embeddings_batch)\n",
    "\n",
    "        # Compute loss\n",
    "        outputs = torch.clamp(outputs, min=-3, max=3)  # Prevent extreme values\n",
    "        loss = criterion(outputs, rasters_batch.unsqueeze(1))  # Add channel dimension to targets\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Logging batch loss\n",
    "        #print(f\"Epoch {epoch + 1}, Batch {batch_idx + 1}/{len(dataloader)}, Loss: {loss.item()}\")\n",
    "\n",
    "        #for name, param in model.named_parameters():\n",
    "        #    if param.requires_grad:\n",
    "        #        print(f\"{name}: max={param.data.max()}, min={param.data.min()}, grad_max={param.grad.max()}, grad_min={param.grad.min()}\")\n",
    "\n",
    "\n",
    "    # Logging epoch loss\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Avg Loss: {total_loss / len(dataloader)}\")\n",
    "    losses.append(total_loss/len(dataloader))\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "epochs_x = np.linspace(1, epochs, epochs)\n",
    "plt.plot(epochs_x, losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be5f746",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb09139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode for one sample\n",
    "model.eval()\n",
    "criterion = nn.BCEWithLogitsLoss(reduction = 'mean')\n",
    "prediction_losses = []\n",
    "\n",
    "for i in range(10,15):\n",
    "    sample_embedding = torch.tensor(test_embs_array1617[i], dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
    "    ground_truth = test_raster_array1617[i].flatten()\n",
    "\n",
    "    # Run inference (no gradients needed for inference)\n",
    "    with torch.no_grad():\n",
    "        predicted_raster = model(sample_embedding)  # Shape of predicted_raster will be [1, 1, 256, 256]\n",
    "        predictions = torch.sigmoid(predicted_raster) > 0.5\n",
    "        actuals = torch.tensor(test_raster_array1617[i].reshape(1, 1, 256, 256))\n",
    "        loss = criterion(predicted_raster, actuals.float())\n",
    "        print(loss)\n",
    "        prediction_losses.append(loss)\n",
    "        print(test_embs_with_polys1617.to_crs(epsg=4326)['geometry'].iloc[i].centroid)\n",
    "\n",
    "\n",
    "    rasterio.plot.show(np.array(predictions))\n",
    "    rasterio.plot.show(test_raster_array1617[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc49756",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_losses_df = pd.DataFrame(np.array(prediction_losses), columns = ['losses'])\n",
    "#plt.hist(prediction_losses_np)\n",
    "\n",
    "mean_value = prediction_losses_df.mean()           # Mean\n",
    "median_value = prediction_losses_df.median()      # Median\n",
    "percentiles = prediction_losses_df.quantile([0.1, 0.5, 0.9])\n",
    "\n",
    "print(\"Mean:\", mean_value)\n",
    "print(\"Median:\", median_value)\n",
    "print(percentiles)\n",
    "\n",
    "\n",
    "smaller_losses = prediction_losses_df[prediction_losses_df < 0.13]\n",
    "plt.hist(smaller_losses)\n",
    "plt.title('90th Percentile Prediction Losses')\n",
    "len(smaller_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a851467",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigger_losses = prediction_losses_np[prediction_losses_np > 0.13]\n",
    "print(bigger_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add2fcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode for train set\n",
    "model.eval()\n",
    "criterion = nn.BCEWithLogitsLoss(reduction = 'mean')\n",
    "train_prediction_losses = []\n",
    "train_all_ground_truths = []\n",
    "train_all_predictions = []\n",
    "\n",
    "for i in range(0,50):\n",
    "    sample_embedding = torch.tensor(train_embs_array1617[i], dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Run inference (no gradients needed for inference)\n",
    "    with torch.no_grad():\n",
    "        predicted_raster = model(sample_embedding)  # Shape of predicted_raster will be [1, 1, 256, 256]\n",
    "        predictions = torch.sigmoid(predicted_raster) > 0.5\n",
    "        actuals = torch.tensor(train_raster_array1617[i].reshape(1, 1, 256, 256))\n",
    "        ground_truth = actuals.squeeze(1).numpy()\n",
    "        loss = criterion(predicted_raster, actuals.float())\n",
    "        print(loss)\n",
    "        prediction_losses.append(loss)\n",
    "        predictions_np = predictions.squeeze(1).numpy()\n",
    "\n",
    "    train_all_predictions.extend(predictions_np)\n",
    "    train_all_ground_truths.extend(ground_truth)\n",
    "\n",
    "    #rasterio.plot.show(np.array(predictions))\n",
    "    #rasterio.plot.show(test_raster_array1617[i])\n",
    "\n",
    "    #print(train_embs_with_polys1617.to_crs(epsg=4326)['geometry'].iloc[i].centroid)\n",
    "\n",
    "# Now `predicted_raster` is your predicted raster output\n",
    "#print(predicted_raster.shape)\n",
    "\n",
    "train_all_predictions = np.array(train_all_predictions)\n",
    "train_all_ground_truths = np.array(train_all_ground_truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7200f7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Predictions Metrics\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "flattened_ground_truths = train_all_ground_truths.flatten().astype(int)  # Convert to 1D and numpy\n",
    "flattened_predictions = train_all_predictions.flatten().astype(int)  # Convert to 1D and numpy\n",
    "\n",
    "precision = precision_score(flattened_ground_truths, flattened_predictions, average='binary')\n",
    "recall = recall_score(flattened_ground_truths, flattened_predictions, average='binary')\n",
    "f1 = f1_score(flattened_ground_truths, flattened_predictions, average='binary')\n",
    "\n",
    "print(f'precision on training: {precision}')\n",
    "print(f'recall on training: {recall}')\n",
    "print(f'f1 on training: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1137557f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode for test set\n",
    "model.eval()\n",
    "criterion = nn.BCEWithLogitsLoss(reduction = 'mean')\n",
    "prediction_losses = []\n",
    "all_ground_truths = []\n",
    "all_predictions = []\n",
    "\n",
    "for i in range(0,50):\n",
    "    sample_embedding = torch.tensor(test_embs_array1617[i], dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Run inference (no gradients needed for inference)\n",
    "    with torch.no_grad():\n",
    "        predicted_raster = model(sample_embedding)  # Shape of predicted_raster will be [1, 1, 256, 256]\n",
    "        predictions = torch.sigmoid(predicted_raster) > 0.5\n",
    "        actuals = torch.tensor(test_raster_array1617[i].reshape(1, 1, 256, 256))\n",
    "        ground_truth = actuals.squeeze(1).numpy()\n",
    "        loss = criterion(predicted_raster, actuals.float())\n",
    "        print(loss)\n",
    "        prediction_losses.append(loss)\n",
    "        predictions_np = predictions.squeeze(1).numpy()\n",
    "\n",
    "    all_predictions.extend(predictions_np)\n",
    "    all_ground_truths.extend(ground_truth)\n",
    "\n",
    "    rasterio.plot.show(np.array(predictions))\n",
    "    rasterio.plot.show(test_raster_array1617[i])\n",
    "\n",
    "    print(test_embs_with_polys1617.to_crs(epsg=4326)['geometry'].iloc[i].centroid)\n",
    "\n",
    "# Now `predicted_raster` is your predicted raster output\n",
    "#print(predicted_raster.shape)\n",
    "\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_ground_truths = np.array(all_ground_truths)\n",
    "\n",
    "print(all_ground_truths.flatten().shape)\n",
    "print(all_predictions.flatten().shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9c9a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "flattened_ground_truths = all_ground_truths.flatten().astype(int)  # Convert to 1D and numpy\n",
    "flattened_predictions = all_predictions.flatten().astype(int)  # Convert to 1D and numpy\n",
    "\n",
    "precision = precision_score(flattened_ground_truths, flattened_predictions, average='binary')\n",
    "recall = recall_score(flattened_ground_truths, flattened_predictions, average='binary')\n",
    "f1 = f1_score(flattened_ground_truths, flattened_predictions, average='binary')\n",
    "\n",
    "print(f'precision on test: {precision}')\n",
    "print(f'recall on test: {recall}')\n",
    "print(f'f1 on test: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99796dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "for i in range(0,10):\n",
    "    sample_embedding = torch.tensor(train_embs_with_polys['embeddings'].iloc[i], dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Run inference (no gradients needed for inference)\n",
    "    with torch.no_grad():\n",
    "        predicted_raster = model(sample_embedding)  # Shape of predicted_raster will be [1, 1, 256, 256]\n",
    "\n",
    "    rasterio.plot.show(np.array(predicted_raster))\n",
    "    rasterio.plot.show(train_raster_array[i])\n",
    "\n",
    "# Now `predicted_raster` is your predicted raster output\n",
    "#print(predicted_raster.shape)\n",
    "\n",
    "#rasterio.plot.show(np.array(predicted_raster))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4237ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack1 = RetrievePixels(q1stack)\n",
    "stack2 = RetrievePixels(q2stack)\n",
    "stack3 = RetrievePixels(q3stack)\n",
    "stack4 = RetrievePixels(q4stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24392326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get median of values across time for different bands for each season\n",
    "import xarray as xr\n",
    "\n",
    "q1_composite = stack1.groupby('band').median(dim = 'time')\n",
    "q2_composite = stack2.groupby('band').median(dim = 'time')\n",
    "q3_composite = stack3.groupby('band').median(dim = 'time')\n",
    "q4_composite = stack4.groupby('band').median(dim = 'time')\n",
    "\n",
    "# Calculate NDWI and create water mask \n",
    "def CalculateNDWI(image):\n",
    "    green = image.sel(band=\"green\")\n",
    "    nir = image.sel(band=\"nir\")\n",
    "\n",
    "    ndwi = (green - nir) / (green + nir)\n",
    "    return ndwi\n",
    "\n",
    "ndwi_q1 = CalculateNDWI(q1_composite)\n",
    "ndwi_q2 = CalculateNDWI(q2_composite)\n",
    "ndwi_q3 = CalculateNDWI(q3_composite)\n",
    "ndwi_q4 = CalculateNDWI(q4_composite)\n",
    "\n",
    "ndwi_ds = [ndwi_q1, ndwi_q2, ndwi_q3, ndwi_q4]\n",
    "\n",
    "for i in range(len(ndwi_ds)):\n",
    "    if 's2:processing_baseline' in ndwi_ds[i].coords:\n",
    "        ndwi_ds[i] = ndwi_ds[i].reset_coords('s2:processing_baseline', drop=True)\n",
    "        \n",
    "annual_median = xr.concat(ndwi_ds, dim=\"time\").median(dim='time')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d137cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rioxarray as rxr\n",
    "\n",
    "# Get water mask, get intersection with pit lakes as positive\n",
    "# Reproject  \n",
    "water_mask = annual_median.rio.write_crs('epsg:32615', inplace=True)\n",
    "water_mask = water_mask.rio.reproject('epsg:4326')\n",
    "water_mask = xr.where(water_mask > 0.1, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02e8303",
   "metadata": {},
   "source": [
    "Label water"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c9b174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-label bounding boxes with water mask\n",
    "# If bbox indicates pit lake & NDWI indicates water, label as positive. Else, negative\n",
    "\n",
    "# If pixel is labeled 1, check whether raster is 1 or 0\n",
    "# Resample water mask to match polygons\n",
    "water_mask_resampled = resample_raster(\n",
    "        src2, \n",
    "        target_transform=transform1,\n",
    "        target_shape=raster1.shape,\n",
    "        resampling=Resampling.nearest\n",
    "    )\n",
    "\n",
    "# Get the corresponding pixel values from the second raster\n",
    "mask = bboxs_raster == 1\n",
    "bboxs_raster[mask] = water_mask_resampled[mask]\n",
    "\n",
    "rasterio.plot.show(bboxs_raster)\n",
    "\n",
    "#rasters.append(bboxs_raster)\n",
    "#embs_with_polys()\n",
    "\n",
    "# Add the statistics to the vector data\n",
    "#testing = mn_qa_bboxs.join(pd.DataFrame(water_mask_chips))\n",
    "#testing.columns\n",
    "#mn_data = mn_data.join(pd.DataFrame(water_mask_chips))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31136412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get chips with pit lakes and regular bodies of water in MN from QA\n",
    "mn_data = LabelEmbeddings(mine_embeddings_mn, lake_embeddings_mn, mn_qa_polys, dnr_water_cropped)\n",
    "print(f\"Total samples: {len(mn_data)}\") \n",
    "\n",
    "\n",
    "# Get chips for features identified as not lakes from QA\n",
    "mn_nonlakes_chips = LabelEmbeddings(dummy, qa_non_mine_embeddings_mn, gpd.GeoDataFrame(), mn_qa_nonlakes)\n",
    "print(f\"Total samples: {len(mn_nonlakes_chips)}\") \n",
    "\n",
    "mn_data = pd.concat([mn_data,mn_nonlakes_chips], ignore_index=True) # Add lakes to MN embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9276c6",
   "metadata": {},
   "source": [
    "### PCA and tSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "aa32389a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to get principle components and plot with data for each embedding\n",
    "\n",
    "def EmbeddingsPCA(data, column = 'embeddings'):\n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters: \n",
    "    data (GeoDataFrame):\n",
    "    column (str): column of data with variable\n",
    "\n",
    "    \"\"\"\n",
    "    # Convert embeddings to array and do PCA\n",
    "    X =  np.vstack(data[column].values)\n",
    "    pca = PCA(n_components=1)\n",
    "    pca.fit(X)\n",
    "\n",
    "    # Check on variance explained by given # of dimensions of PCA\n",
    "    print(\"Relative variance in principal components:\", pca.explained_variance_ratio_)\n",
    "\n",
    "    # Apply PCA to get first two dimensions\n",
    "    data['pca1'] = pca.transform(X)[:, 0]\n",
    "    #data['pca2'] = pca.transform(X)[:, 1]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b4c4c51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCAVisualization(data, column, label1, label2):\n",
    "    # Visualize the first two principal components with category color-coding\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Plot for category 1\n",
    "    ax.scatter(data[data[column] == True]['pca1'], \n",
    "                data[data[column] == True]['pca2'], \n",
    "                color='blue', label=label1, alpha=0.5)\n",
    "\n",
    "    # Plot for category 2\n",
    "    ax.scatter(data[data[column] == False]['pca1'], \n",
    "                data[data[column] == False]['pca2'], \n",
    "                color='grey', label=label2, alpha=0.05)\n",
    "    \n",
    "    \"\"\" EDIT SO CAN SEE WHAT THE LITTLE OVERLAP IS  plt.scatter(data[data['wb_'] == new_condition]['pca1'], \n",
    "            data[data[new_class_column] == new_condition]['pca2'], \n",
    "            color='red', label=label3, alpha=0.5, marker='^') \"\"\"\n",
    "\n",
    "    plt.xlabel('PCA 1')\n",
    "    plt.ylabel('PCA 2')\n",
    "    ax.legend()\n",
    "    plt.title('PCA of NAIP Embeddings')\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7604bc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "mn_pca = EmbeddingsPCA(mn_data)\n",
    "PCAVisualization(mn_pca, column = 'mine', label1 = 'Pit lakes', label2 = 'Non-pit lake water bodies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3413f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "az_pca = EmbeddingsPCA(az_data)\n",
    "ax = PCAVisualization(az_pca, 'mine','Pit lakes', 'No pit lakes present')\n",
    "\n",
    "end_idx = len(az_pca)-1\n",
    "start_idx = end_idx-len(az_lakes_chips)\n",
    "\n",
    "ax.scatter(az_pca[start_idx:end_idx]['pca1'], \n",
    "                az_pca[start_idx:end_idx]['pca2'],\n",
    "                color='green', label = 'lake', alpha = 0.1)\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08216096",
   "metadata": {},
   "outputs": [],
   "source": [
    "#t-SNE of positive/negative embeddings\n",
    "\n",
    "labels = data['mine'].values\n",
    "\n",
    "tsne_data = np.vstack(data['embeddings'].values)\n",
    "\n",
    "# Set up t-SNE\n",
    "tsne = TSNE(n_components=2, perplexity=25, n_iter=250)\n",
    "reduced_embeddings = tsne.fit_transform(tsne_data)\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "custom_labels = {0: \"Not Pit Lakes\", 1: \"Pit Lakes\"}\n",
    "\n",
    "for label in np.unique(labels):\n",
    "    mask = labels == label\n",
    "    plt.scatter(reduced_embeddings[mask, 1],\n",
    "                reduced_embeddings[mask, 0],\n",
    "                label=custom_labels.get(label, f\"Label {label}\"),\n",
    "                alpha=0.7)\n",
    "\n",
    "plt.title(\"t-SNE of Minnesota Embeddings\")\n",
    "plt.xlabel(\"Dimension 1\")\n",
    "plt.ylabel(\"Dimension 2\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b4f14c",
   "metadata": {},
   "source": [
    "### Build classification model to predict presence of pit lakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 909,
   "id": "f0aea2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark positive samples \n",
    "import math\n",
    "\n",
    "def MarkSamples(data, polygons, pct):\n",
    "    \"\"\" Mark as positive if intersection is greater than given % of area.\n",
    "    \"\"\"\n",
    "    # Get polygon of intersection for each chip\n",
    "    positive_data = data[data['mine'] == 1]\n",
    "    negative_data = data[data['mine'] == 0]\n",
    "    positive_data['emb_idx'] = positive_data.index\n",
    "    intersecting = gpd.overlay(positive_data, polygons, how = 'intersection') # Returns polygons of intersection\n",
    "    intersecting = intersecting.dissolve(by='emb_idx') # Makes intersecting index match with data, adds areas of intersection\n",
    "    intersecting['overlapping_area'] = intersecting.geometry.area\n",
    "\n",
    "    # Merge into one dataframe \n",
    "    positive_data = positive_data.merge(intersecting['overlapping_area'], left_index = True, right_index = True, how='left')\n",
    "\n",
    "    # Check intersection % and mark True/False\n",
    "    for index, row in positive_data.iterrows():\n",
    "        pct_overlap = row['overlapping_area'] / row.geometry.area\n",
    "        positive_data.loc[index,'pct_pitlake'] = pct_overlap\n",
    "        if pct_overlap >= pct:\n",
    "            positive_data['mine'][index] = 1\n",
    "        elif math.isnan(row['overlapping_area']):\n",
    "            positive_data['mine'][index] = 0\n",
    "        else: \n",
    "            positive_data['mine'][index]  = 0\n",
    "\n",
    "    data = pd.concat([positive_data, negative_data])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec4178a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Larger US\n",
    "\n",
    "# Get 16 and 17\n",
    "usdata1617 = pd.concat([data16R, data16S, data16T, data17R, data17S, data17T])\n",
    "usdata1617 = usdata1617.drop_duplicates('geometry')\n",
    "\n",
    "print(len(us_qa_pits_16)+len(us_qa_pits_17))\n",
    "us_qa_pits_1617 = pd.concat([us_qa_pits_16, us_qa_pits_17]).drop_duplicates('geometry')\n",
    "print(len(us_qa_pits_1617))\n",
    "\n",
    "# Check if intersection of each chip with a pit lake polygon is > 0.03%\n",
    "usdata1617marked = MarkSamples(usdata1617, us_qa_pits_1617, 0.0003)\n",
    "print((usdata1617marked['mine']==1).sum())\n",
    "print(len(usdata1617marked))\n",
    "\n",
    "# Split into train and test\n",
    "usdata1617marked = usdata1617marked.reset_index(drop=True)\n",
    "us1617_train_embeddings = np.vstack(usdata1617marked['embeddings'].values)\n",
    "us1617_labels = np.array(usdata1617marked['mine'])\n",
    "us1617_indices = usdata1617marked.index\n",
    "\n",
    "us1617_X_train, us1617_X_test, us1617_y_train, us1617_y_test, us1617_train_idx, us1617_test_idx = sklearn.model_selection.train_test_split(us1617_train_embeddings, us1617_labels, us1617_indices, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)\n",
    "\n",
    "print(np.shape(us1617_X_train))\n",
    "print(np.shape(us1617_X_test))\n",
    "print(np.shape(us1617_y_train))\n",
    "print(np.shape(us1617_y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2c72b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "usdata1617marked['pct_pitlake'].fillna(0, inplace=True)\n",
    "usdata1617marked['pct_pitlake']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294cc1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Larger US REGRESSOR\n",
    "\n",
    "# Split into train and test\n",
    "usdata1617marked = usdata1617marked.reset_index(drop=True)\n",
    "us1617_train_embeddings = np.vstack(usdata1617marked['embeddings'].values)\n",
    "us1617_labels = np.array(usdata1617marked['pct_pitlake'])\n",
    "us1617_indices = usdata1617marked.index\n",
    "\n",
    "us1617_X_train, us1617_X_test, us1617_y_train, us1617_y_test, us1617_train_idx, us1617_test_idx = sklearn.model_selection.train_test_split(us1617_train_embeddings, us1617_labels, us1617_indices, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)\n",
    "\n",
    "print(np.shape(us1617_X_train))\n",
    "print(np.shape(us1617_X_test))\n",
    "print(np.shape(us1617_y_train))\n",
    "print(np.shape(us1617_y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 950,
   "id": "e9093567",
   "metadata": {},
   "outputs": [],
   "source": [
    "#usdata1617marked = usdata1617marked[usdata1617marked['mine'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2f94fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "usdata1617marked['pct_pitlake'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aec1166",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(usdata1617marked['pct_pitlake'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e184ed8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Create and train the RandomForestRegressor\n",
    "regressor = RandomForestRegressor(n_estimators=100, random_state=1)\n",
    "regressor.fit(us1617_X_train, us1617_y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = regressor.predict(us1617_X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(us1617_y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "math.sqrt(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168efb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.xlim(0,0.08)\n",
    "plt.ylim(0,0.08)\n",
    "ax.scatter(regressor.predict(us1617_X_train), us1617_y_train, color = 'grey')\n",
    "ax.scatter(y_pred, us1617_y_test, color = 'blue')\n",
    "x = np.linspace(0,1)\n",
    "y = np.linspace(0,1)\n",
    "ax.plot(x, y, linestyle = ':')\n",
    "ax.legend(['train', 'test'])\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('True')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eefe940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train neural network model for IN and MN\n",
    "#from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "#class_weights = compute_class_weight('balanced', classes=np.array([0, 1]), y=mn_y_train)\n",
    "\n",
    "# Convert the class weights to a dictionary\n",
    "#class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "\n",
    "us1617_clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(16, 16, 16), max_iter = 1000, \n",
    "                    random_state=1)\n",
    "\n",
    "us1617_clf.fit(us1617_X_train, us1617_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0ea0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run predictions on test set and get F1 scores and probabilities\n",
    "\n",
    "us1617_predictions = us1617_clf.predict(us1617_X_test) \n",
    "print(f'Number of positive predictions: {us1617_predictions.sum()}')\n",
    "print(f'Number of positive samples: {us1617_y_test.sum()}')\n",
    "print(f'Number of test samples: {len(us1617_X_test)}')\n",
    "\n",
    "f1 = sklearn.metrics.f1_score(us1617_y_test, us1617_predictions)\n",
    "print(f'F1: {f1}')\n",
    "\n",
    "in_mn_precision = sklearn.metrics.precision_score(us1617_y_test, us1617_predictions)\n",
    "print(f'precision: {in_mn_precision}')\n",
    "\n",
    "in_mn_recall = sklearn.metrics.recall_score(us1617_y_test, us1617_predictions)\n",
    "print(f'recall: {in_mn_recall}')\n",
    "\n",
    "us1617_probabilities = us1617_clf.predict_proba(us1617_X_test)\n",
    "print(f'Classes for probability: {us1617_clf.classes_}')\n",
    "\n",
    "us1617_probabilities[:,1] # Probability of pit lake presence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4354a0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write predictions and probabilities to shapefiles\n",
    "in_mn_pits_geom = in_mn_data_marked_reset.loc[in_mn_test_idx].geometry.reset_index(drop = True)\n",
    "\n",
    "in_mn_pits_pred = gpd.GeoDataFrame(in_mn_predictions, columns = ['mines'], geometry=in_mn_pits_geom, crs = 'EPSG:4326')\n",
    "in_mn_pits_probs = gpd.GeoDataFrame(in_mn_probabilities[:,1], columns = ['mine_prob'], geometry=in_mn_pits_geom, crs = 'EPSG:4326')\n",
    "\n",
    "in_mn_pits_probs.to_file('north_pits_probs_0112')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335bfe03",
   "metadata": {},
   "source": [
    "#### Minnesota & Indiana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59d1793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get MN, IN, KY data\n",
    "in_mn_data = pd.concat([in_data, mn_data])\n",
    "north_data = pd.concat([in_mn_data, ky_data])\n",
    "in_mn_pits = pd.concat([mn_aggregated_pits, in_qa_pits])\n",
    "north_pits = pd.concat([in_mn_pits, ky_qa_pits])\n",
    "\n",
    "# Check if intersection of each chip with a pit lake polygon is > 0.5%\n",
    "in_mn_data_marked = MarkSamples(north_data, north_pits, 0.006)\n",
    "print((in_mn_data_marked['mine']==1).sum())\n",
    "print(len(in_mn_data_marked))\n",
    "\n",
    "# Split into train and test\n",
    "in_mn_data_marked_reset = in_mn_data_marked.reset_index(drop=True)\n",
    "in_mn_train_embeddings = np.vstack(in_mn_data_marked_reset['embeddings'].values)\n",
    "in_mn_labels = np.array(in_mn_data_marked_reset['mine'])\n",
    "in_mn_indices = in_mn_data_marked_reset.index\n",
    "\n",
    "in_mn_X_train, in_mn_X_test, in_mn_y_train, in_mn_y_test, in_mn_train_idx, in_mn_test_idx = sklearn.model_selection.train_test_split(in_mn_train_embeddings, in_mn_labels, in_mn_indices, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)\n",
    "\n",
    "print(np.shape(in_mn_X_train))\n",
    "print(np.shape(in_mn_X_test))\n",
    "print(np.shape(in_mn_y_train))\n",
    "print(np.shape(in_mn_y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6ee396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train neural network model for IN and MN\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "#class_weights = compute_class_weight('balanced', classes=np.array([0, 1]), y=mn_y_train)\n",
    "\n",
    "# Convert the class weights to a dictionary\n",
    "#class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "\n",
    "in_mn_clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(16, 16, 16), max_iter = 1000, \n",
    "                    random_state=1)\n",
    "\n",
    "in_mn_clf.fit(in_mn_X_train, in_mn_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756a2539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run predictions on test set and get F1 scores and probabilities\n",
    "\n",
    "in_mn_predictions = in_mn_clf.predict(in_mn_X_test) \n",
    "print(f'Number of positive predictions: {in_mn_predictions.sum()}')\n",
    "print(f'Number of positive samples: {in_mn_y_test.sum()}')\n",
    "print(f'Number of test samples: {len(in_mn_X_test)}')\n",
    "\n",
    "f1 = sklearn.metrics.f1_score(in_mn_y_test, in_mn_predictions)\n",
    "print(f'F1: {f1}')\n",
    "\n",
    "in_mn_precision = sklearn.metrics.precision_score(in_mn_y_test, in_mn_predictions)\n",
    "print(f'precision: {in_mn_precision}')\n",
    "\n",
    "in_mn_recall = sklearn.metrics.recall_score(in_mn_y_test, in_mn_predictions)\n",
    "print(f'recall: {in_mn_recall}')\n",
    "\n",
    "in_mn_probabilities = in_mn_clf.predict_proba(in_mn_X_test)\n",
    "print(f'Classes for probability: {in_mn_clf.classes_}')\n",
    "\n",
    "in_mn_probabilities[:,1] # Probability of pit lake presence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1161,
   "id": "d3fc7dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write predictions and probabilities to shapefiles\n",
    "in_mn_pits_geom = in_mn_data_marked_reset.loc[in_mn_test_idx].geometry.reset_index(drop = True)\n",
    "\n",
    "in_mn_pits_pred = gpd.GeoDataFrame(in_mn_predictions, columns = ['mines'], geometry=in_mn_pits_geom, crs = 'EPSG:4326')\n",
    "in_mn_pits_probs = gpd.GeoDataFrame(in_mn_probabilities[:,1], columns = ['mine_prob'], geometry=in_mn_pits_geom, crs = 'EPSG:4326')\n",
    "\n",
    "in_mn_pits_probs.to_file('north_pits_probs_0112')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c05c7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(in_mn_clf, 'data/NorthPitLakeClf0112.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93f9325",
   "metadata": {},
   "source": [
    "#### Minnesota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f51b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if intersection of each chip with a pit lake polygon is > 0.5%\n",
    "mn_data_marked = MarkSamples(mn_data, mn_aggregated_pits, 0.005)\n",
    "print((mn_data_marked['mine']==1).sum())\n",
    "print(len(mn_data_marked))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c49386",
   "metadata": {},
   "outputs": [],
   "source": [
    "mn_data_marked.to_file('mn_coverage_check_112')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7a6e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test for MN\n",
    "\n",
    "mn_data_marked_reset = mn_data_marked.reset_index(drop=True)\n",
    "mn_train_embeddings = np.vstack(mn_data_marked_reset['embeddings'].values)\n",
    "mn_labels = np.array(mn_data_marked_reset['mine'])\n",
    "mn_indices = mn_data_marked_reset.index\n",
    "\n",
    "mn_X_train, mn_X_test, mn_y_train, mn_y_test, mn_train_idx, mn_test_idx = sklearn.model_selection.train_test_split(mn_train_embeddings, mn_labels, mn_indices, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)\n",
    "\n",
    "print(np.shape(mn_X_train))\n",
    "print(np.shape(mn_X_test))\n",
    "print(np.shape(mn_y_train))\n",
    "print(np.shape(mn_y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a164a04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train neural network model\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "#class_weights = compute_class_weight('balanced', classes=np.array([0, 1]), y=mn_y_train)\n",
    "\n",
    "# Convert the class weights to a dictionary\n",
    "#class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "\n",
    "mn_clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(32, 32, 32), max_iter = 1000, \n",
    "                    random_state=1)\n",
    "\n",
    "mn_clf.fit(mn_X_train, mn_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015ef56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run predictions on test set and get F1 scores and probabilities\n",
    "\n",
    "mn_predictions = mn_clf.predict(mn_X_test) \n",
    "print(f'Number of positive predictions: {mn_predictions.sum()}')\n",
    "print(f'Number of positive samples: {mn_y_test.sum()}')\n",
    "print(f'Number of test samples: {len(mn_X_test)}')\n",
    "\n",
    "f1 = sklearn.metrics.f1_score(mn_y_test, mn_predictions)\n",
    "print(f'F1: {f1}')\n",
    "\n",
    "mn_precision = sklearn.metrics.precision_score(mn_y_test, mn_predictions)\n",
    "print(f'precision: {mn_precision}')\n",
    "\n",
    "mn_probabilities = mn_clf.predict_proba(mn_X_test)\n",
    "print(f'Classes for probability: {mn_clf.classes_}')\n",
    "\n",
    "mn_probabilities[:,1] # Probability of pit lake presence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c90eaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(mn_clf, 'data/MNPitLakeClf0109.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 966,
   "id": "ce935043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write predictions and probabilities to shapefiles\n",
    "mn_pits_geom = mn_data_marked_reset.loc[mn_test_idx].geometry.reset_index(drop = True)\n",
    "\n",
    "mn_pits_pred = gpd.GeoDataFrame(mn_predictions, columns = ['mines'], geometry=mn_pits_geom, crs = 'EPSG:4326')\n",
    "mn_pits_probs = gpd.GeoDataFrame(mn_probabilities[:,1], columns = ['mine_prob'], geometry=mn_pits_geom, crs = 'EPSG:4326')\n",
    "\n",
    "mn_pits_probs.to_file('mn_pits_probs_0112')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aef464",
   "metadata": {},
   "outputs": [],
   "source": [
    "mn_train_predictions = mn_clf.predict(mn_X_train)\n",
    "\n",
    "mn_f1_train = sklearn.metrics.f1_score(mn_y_train, mn_train_predictions)\n",
    "print(f'F1 on training data: {mn_f1_train}')\n",
    "\n",
    "mn_train_accuracy = sklearn.metrics.accuracy_score(mn_y_train, mn_train_predictions)\n",
    "mn_test_accuracy = sklearn.metrics.accuracy_score(mn_y_test, mn_predictions)\n",
    "print(f'Accuracy on training data: {mn_train_accuracy}')\n",
    "print(f'Accuracy on test data: {mn_test_accuracy}')\n",
    "\n",
    "# Plot histogram of probability predictions on test set\n",
    "test_probs = pd.DataFrame(mn_probabilities)\n",
    "bins = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "test_probs[1].hist(bins = bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360103ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings for Crosby, MN\n",
    "crosby_pitlakes = gpd.read_file('data/crosby_mn.geojson') # all Crosby\n",
    "\n",
    "crosby_embeddings, dummy = GetEmbeddingsFromIntersection1_5(folder_path = 'data/MN_v1_5/', polygons = crosby_pitlakes, random_images = False)\n",
    "\n",
    "print(\"Number of images:\")\n",
    "print(len(crosby_embeddings))\n",
    "\n",
    "# Get Crosby test chips\n",
    "crosby_embeddings = pd.DataFrame(crosby_embeddings)\n",
    "crosby_embs_avg = MonthlyComposite(crosby_embeddings)\n",
    "print(len(crosby_embs_avg))\n",
    "crosby_test_chips = LabelEmbeddings1_5(crosby_embs_avg, dummy, crosby_pitlakes) #  getting all Crosby, not just pit lakes\n",
    "\n",
    "print(\"Total samples:\") \n",
    "print(len(crosby_test_chips))\n",
    "\n",
    "crosby_test = np.vstack(crosby_test_chips['embeddings'].values) \n",
    "\n",
    "crosby_test_preds = gpd.GeoDataFrame(mn_clf.predict(crosby_test), columns = ['mine'], geometry = crosby_test_chips['geometry'])\n",
    "crosby_test_probs = gpd.GeoDataFrame(mn_clf.predict_proba(crosby_test)[:,1], columns = ['mine_prob'], geometry=crosby_test_chips['geometry'], crs = 'EPSG:4326')\n",
    "\n",
    "crosby_test_probs.plot(column = 'mine_prob', legend = True)\n",
    "\n",
    "# Write Crosby predictions to shapefile\n",
    "crosby_test_probs.to_file('crosby_testing_0112')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd8de4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with IN and MN combined model\n",
    "\n",
    "crosby_test_preds = gpd.GeoDataFrame(in_mn_clf.predict(crosby_test), columns = ['mine'], geometry = crosby_test_chips['geometry'])\n",
    "crosby_test_probs = gpd.GeoDataFrame(in_mn_clf.predict_proba(crosby_test)[:,1], columns = ['mine_prob'], geometry=crosby_test_chips['geometry'], crs = 'EPSG:4326')\n",
    "\n",
    "crosby_test_probs.plot(column = 'mine_prob', legend = True)\n",
    "\n",
    "# Write Crosby predictions to shapefile\n",
    "#crosby_test_probs.to_file('crosby_testing_0112_no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39faf954",
   "metadata": {},
   "outputs": [],
   "source": [
    "(crosby_test_preds['mine']==1).sum()\n",
    "\n",
    "dnr_pit_lakes_crosby = mn_water_features.loc[mn_water_features['wb_class'].isin(mine_classes)].to_crs(epsg=4326)\n",
    "\n",
    "crosby_pits = gpd.sjoin(dnr_pit_lakes_crosby, crosby_pitlakes, how='inner', predicate = 'intersects')\n",
    "\n",
    "crosby_marked = MarkSamples(crosby_test_chips, crosby_pits, 0.002)\n",
    "\n",
    "print(crosby_marked['mine'].sum())\n",
    "len(crosby_test_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72a7436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings for MI\n",
    "mi_qa = gpd.read_file('data/mi_testing.geojson')\n",
    "\n",
    "mi_qa_embeddings, dummy = GetEmbeddingsFromIntersection(folder_path = 'data/60cm_mi/rgbir_cog/', polygons = mi_qa, random_images = False)\n",
    "\n",
    "print(\"Number of images:\")\n",
    "print(len(mi_qa_embeddings))\n",
    "\n",
    "# Get MI test chips\n",
    "mi_test_chips = LabelEmbeddings(mi_qa_embeddings, dummy, mi_qa)\n",
    "\n",
    "print(\"Total samples:\") \n",
    "print(len(mi_test_chips))\n",
    "\n",
    "mi_test = np.vstack(mi_test_chips['embeddings'].values) \n",
    "\n",
    "mi_test_preds = gpd.GeoDataFrame(mn_clf.predict(mi_test), columns = ['mine'], geometry = mi_test_chips['geometry'])\n",
    "mi_test_probs = gpd.GeoDataFrame(mn_clf.predict_proba(mi_test)[:,1], columns = ['mine_prob'], geometry=mi_test_chips['geometry'], crs = 'EPSG:4326')\n",
    "\n",
    "# Write Crosby predictions to shapefile\n",
    "mi_test_probs.to_file('mi_testing_1218')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92da2019",
   "metadata": {},
   "source": [
    "#### Test on MI lakes vs. pit lakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4395a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get lake data - filter out pit lakes\n",
    "\n",
    "mi_lakes = gpd.read_file('data/mi_Lake_Polygons.geojson').to_crs(epsg = 4326)\n",
    "states = gpd.read_file('data/state_boundaries')\n",
    "mi = states[states['STUSPS']=='MI']\n",
    "mi = mi.to_crs(epsg = 4326).geometry.unary_union \n",
    "mi_qa = US_QA[US_QA.geometry.intersects(mi)]\n",
    "\n",
    "mi_qa_pits = mi_qa[mi_qa['category'].isin(['a','q'])] # QAed pit lakes and questionable\n",
    "mi_lakes_polys = gpd.overlay(mi_lakes, mi_qa_pits, how = 'difference')\n",
    "print(len(mi_lakes_polys))\n",
    "\n",
    "# Get pit lake polygon intersections - keep polygons within pit lakes; otherwise keep bbox\n",
    "mi_qa_polys = gpd.overlay(mi_qa_pits, mi_lakes, how = 'intersection')\n",
    "\n",
    "bboxs_with_polygon = gpd.sjoin(mi_qa_pits, mi_lakes, how=\"left\", predicate=\"intersects\")\n",
    "print(len(bboxs_with_polygon))\n",
    "\n",
    "bboxs_without_polygon = bboxs_with_polygon[bboxs_with_polygon.index_right.isna()]\n",
    "print(len(bboxs_without_polygon))\n",
    "\n",
    "mi_qa_polys = pd.concat([mi_qa_polys, bboxs_without_polygon])\n",
    "mi_qa_polys.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcc6173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get intersecting embeddings, label these\n",
    "mi_embeddings, dummy = GetEmbeddingsFromIntersection(folder_path = 'data/60cm_mi/rgbir_cog/', polygons = mi_qa_polys, random_images = False)\n",
    "\n",
    "print(\"Number of mine images:\")\n",
    "print(len(mi_embeddings))\n",
    "\n",
    "# Crop\n",
    "random_idx = random.sample(range(1, len(mi_lakes_polys)), 500)\n",
    "mi_lakes_cropped = mi_lakes_polys.iloc[random_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2efef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get lake intersecting embeddings\n",
    "mi_water_embeddings, dummy = GetEmbeddingsFromIntersection(folder_path = 'data/60cm_mi/rgbir_cog/', polygons = mi_lakes_cropped, random_images = False)\n",
    "print(f\"Number of water images: {len(mi_embeddings)}\")\n",
    "\n",
    "# Get MI test chips\n",
    "mi_test_chips = LabelEmbeddings(mi_embeddings, mi_water_embeddings, mi_qa_polys, mi_lakes_cropped)\n",
    "\n",
    "print(\"Total samples:\") \n",
    "print(len(mi_test_chips))\n",
    "\n",
    "\n",
    "# Predict on these\n",
    "mi_test = np.vstack(mi_test_chips['embeddings'].values) \n",
    "mi_test_labels = mi_test_chips['mine']\n",
    "\n",
    "mi_test_preds = gpd.GeoDataFrame(mn_clf.predict(mi_test), columns = ['mine'], geometry = mi_test_chips['geometry'])\n",
    "mi_test_probs = gpd.GeoDataFrame(mn_clf.predict_proba(mi_test)[:,1], columns = ['mine_prob'], geometry=mi_test_chips['geometry'], crs = 'EPSG:4326')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa122c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_test_preds_list = mn_clf.predict(mi_test)\n",
    "\n",
    "mi_precision = sklearn.metrics.precision_score(mi_test_labels, mi_test_preds_list)\n",
    "print(f'precision: {mi_precision}')\n",
    "\n",
    "mi_f1 = sklearn.metrics.f1_score(mi_test_labels, mi_test_preds_list)\n",
    "print(f'F1 on MI: {mi_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f654ddb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write MI predictions to shapefile\n",
    "mi_test_probs.to_file('mi_testing_1219_2p')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91a169f",
   "metadata": {},
   "source": [
    "#### Arizona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aac9ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark as positive if intersection > 30% \n",
    "az_data_marked_reset = az_data.reset_index(drop=True)\n",
    "az_data_marked = MarkSamples(az_data_marked_reset, az_qa_pits, 0.003)\n",
    "\n",
    "print((az_data_marked['mine']==1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa99eed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test for AZ\n",
    "\n",
    "az_train_embeddings =  np.vstack(az_data_marked['embeddings'].values)\n",
    "az_labels = np.array(az_data_marked['mine'])\n",
    "az_indices = az_data_marked.index\n",
    "\n",
    "az_X_train, az_X_test, az_y_train, az_y_test, az_train_idx, az_test_idx = sklearn.model_selection.train_test_split(az_train_embeddings, az_labels, az_indices, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)\n",
    "\n",
    "print(np.shape(az_X_train))\n",
    "print(np.shape(az_X_test))\n",
    "print(np.shape(az_y_train))\n",
    "print(np.shape(az_y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972403a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train neural network model\n",
    "\n",
    "az_clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(16, 16, 16), max_iter = 1000, random_state=1)\n",
    "\n",
    "az_clf.fit(az_X_train, az_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c5655e",
   "metadata": {},
   "outputs": [],
   "source": [
    "az_predictions = az_clf.predict(az_X_test) \n",
    "print(f'Number of positive predictions: {az_predictions.sum()}')\n",
    "print(f'Number of test samples: {len(az_X_test)}')\n",
    "\n",
    "az_f1 = sklearn.metrics.f1_score(az_y_test, az_predictions)\n",
    "print(f'F1: {f1}')\n",
    "\n",
    "probabilities = az_clf.predict_proba(az_X_test)\n",
    "print(f'Classes for probability: {az_clf.classes_}')\n",
    "\n",
    "probabilities[:,1] # Probability of pit lake presence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3efd3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "az_train_predictions = az_clf.predict(az_X_train)\n",
    "\n",
    "az_f1_train = sklearn.metrics.f1_score(az_y_train, az_train_predictions)\n",
    "print(f'F1 on training data: {az_f1_train}')\n",
    "\n",
    "train_accuracy = sklearn.metrics.accuracy_score(az_y_train, az_train_predictions)\n",
    "test_accuracy = sklearn.metrics.accuracy_score(az_y_test, az_predictions)\n",
    "print(f'Accuracy on training data: {train_accuracy}')\n",
    "print(f'Accuracy on test data: {test_accuracy}')\n",
    "\n",
    "# Plot histogram of probability predictions on test set\n",
    "test_probs = pd.DataFrame(probabilities)\n",
    "bins = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "test_probs[1].hist(bins = bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae29024c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write predictions and probabilities to shapefiles\n",
    "az_pits_geom = az_data.geometry.loc[az_test_idx].reset_index(drop = True)\n",
    "az_pits_pred = gpd.GeoDataFrame(az_predictions, columns = ['mines'], geometry=az_pits_geom, crs = 'EPSG:4326')\n",
    "az_pits_probs = gpd.GeoDataFrame(probabilities[:,1], columns = ['mine_prob'], geometry=az_pits_geom, crs = 'EPSG:4326')\n",
    "\n",
    "#az_pits_pred.to_file('az_pits_pred_1')\n",
    "az_pits_probs.to_file('az_pits_probs_0108')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9aabee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings for Miami, AZ pitlakes\n",
    "miami_pitlakes = gpd.read_file('data/miami_az_pits.geojson')\n",
    "\n",
    "miami_embeddings, dummy = GetEmbeddingsFromIntersection(folder_path = 'data/60cm_az/rgbir_cog/', polygons = miami_pitlakes, random_images = False)\n",
    "\n",
    "print(\"Number of images:\")\n",
    "print(len(miami_embeddings))\n",
    "\n",
    "# Get test chips in AZ\n",
    "miami_test_chips = LabelEmbeddings(miami_embeddings, dummy, miami_pitlakes)\n",
    "\n",
    "print(\"Total samples:\") \n",
    "print(len(miami_test_chips))\n",
    "\n",
    "miami_test = np.vstack(miami_test_chips['embeddings'].values) \n",
    "\n",
    "miami_test_preds = gpd.GeoDataFrame(az_clf.predict(miami_test), columns = ['mine'], geometry = miami_test_chips['geometry'])\n",
    "miami_test_probs = gpd.GeoDataFrame(az_clf.predict_proba(miami_test)[:,1], columns = ['mine_prob'], geometry=miami_test_chips['geometry'], crs = 'EPSG:4326')\n",
    "\n",
    "miami_test_probs.to_file('miami_az_testing_heldout_1216')\n",
    "miami_test_probs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
